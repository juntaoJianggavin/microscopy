{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sperm segmentation.ipynb",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr12R7eL-E6r",
        "outputId": "e96f3e53-8548-4bdd-f9f4-16d0463c81bc"
      },
      "source": [
        "!pip install roifile"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting roifile\n",
            "  Downloading roifile-2021.6.6-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from roifile) (1.19.5)\n",
            "Installing collected packages: roifile\n",
            "Successfully installed roifile-2021.6.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSyR_6wo6iTW"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo3obb1oAAwG",
        "outputId": "c6c0a065-0f89-4c9f-a628-308a8a91970a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DFR3g92v7-rQ",
        "outputId": "d5139821-175f-401b-f810-61771ab587d9"
      },
      "source": [
        "!python3 --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.7.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-mK28uyAU8U"
      },
      "source": [
        "from skimage import io\n",
        "import os \n",
        "from roifile import ImagejRoi\n",
        "import tifffile\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lcEoibTbnsgT"
      },
      "source": [
        "#Produce Masks from ROI file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plWLB_2x_sMm"
      },
      "source": [
        "src=\"/content/drive/MyDrive/Colab Notebooks/spermdata/spermdata/ROI/C2-Image 39_RoiSet\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-NQTleKAopc"
      },
      "source": [
        "os.chdir(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7orUSmi6-Wn7",
        "outputId": "ba7b0350-79ac-43ab-a9e6-c57f588c0648"
      },
      "source": [
        "list_coordinates = []\n",
        "for root, dirs, files in os.walk(src):\n",
        "  for file in files:\n",
        "    roi = ImagejRoi.fromfile(file)\n",
        "    list_x=roi.coordinates()[:,0]\n",
        "    list_y=roi.coordinates()[:,1]\n",
        "    list_xy=[]\n",
        "    for i in range(0,len (list_x)-1):\n",
        "      coordinate=[list_x[i],list_y[i]]\n",
        "      list_xy.append(coordinate)\n",
        "    list_coordinates.append(list_xy)\n",
        "print(list_coordinates)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[403.0, 191.0], [402.3444, 191.55669], [401.59482, 192.0102], [400.757, 192.35707], [399.8373, 192.59468], [398.84277, 192.72122], [397.7809, 192.73573], [396.65985, 192.63809], [395.48813, 192.42903], [394.2746, 192.11018], [393.02856, 191.68393], [391.75946, 191.15353], [390.47696, 190.52304], [389.19086, 189.79723], [387.9109, 188.98164], [386.64682, 188.08247], [385.4083, 187.10657], [384.20468, 186.06136], [383.0452, 184.9548], [381.93863, 183.79532], [380.89343, 182.59172], [379.91754, 181.35318], [379.01837, 180.08911], [378.20276, 178.80916], [377.47696, 177.52304], [376.84647, 176.24055], [376.31607, 174.97145], [375.88983, 173.7254], [375.57095, 172.51189], [375.3619, 171.34013], [375.26428, 170.21907], [375.27878, 169.15723], [375.4053, 168.16267], [375.6429, 167.243], [375.9898, 166.40518], [376.44333, 165.65562], [377.0, 165.0], [377.6556, 164.44331], [378.40518, 163.9898], [379.243, 163.64293], [380.1627, 163.40532], [381.15723, 163.27878], [382.2191, 163.26427], [383.34015, 163.36191], [384.51187, 163.57097], [385.7254, 163.88982], [386.97144, 164.31607], [388.24054, 164.84647], [389.52304, 165.47696], [390.80914, 166.20277], [392.0891, 167.01836], [393.35318, 167.91753], [394.5917, 168.89343], [395.79532, 169.93864], [396.9548, 171.0452], [398.06137, 172.20468], [399.10657, 173.40828], [400.08246, 174.64682], [400.98163, 175.91089], [401.79724, 177.19084], [402.52304, 178.47696], [403.15353, 179.75945], [403.68393, 181.02855], [404.11017, 182.2746], [404.42905, 183.48811], [404.6381, 184.65987], [404.73572, 185.78093], [404.72122, 186.84277], [404.5947, 187.83733], [404.3571, 188.757], [404.0102, 189.59482]], [[450.0, 256.0], [449.18784, 255.88678], [448.38947, 255.64124], [447.61096, 255.26524], [446.85825, 254.76164], [446.13705, 254.1343], [445.45285, 253.38795], [444.81088, 252.5283], [444.21603, 251.56189], [443.6728, 250.49608], [443.18533, 249.33896], [442.75732, 248.09935], [442.39206, 246.78668], [442.09232, 245.41096], [441.86035, 243.98264], [441.69797, 242.51257], [441.60635, 241.012], [441.58624, 239.49228], [441.63776, 237.96501], [441.76056, 236.44182], [441.95364, 234.9343], [442.2156, 233.45392], [442.5444, 232.01193], [442.93756, 230.61932], [443.39206, 229.28668], [443.90448, 228.02419], [444.4709, 226.8414], [445.087, 225.74734], [445.7481, 224.75034], [446.4492, 223.85799], [447.1849, 223.07706], [447.94965, 222.41351], [448.7376, 221.8724], [449.54282, 221.45784], [450.35907, 221.17296], [451.18024, 221.01996], [452.0, 221.0], [452.81216, 221.11322], [453.61053, 221.35876], [454.38904, 221.73476], [455.14175, 222.23836], [455.86295, 222.8657], [456.54715, 223.61205], [457.18912, 224.4717], [457.78397, 225.43811], [458.3272, 226.50392], [458.81467, 227.66104], [459.24268, 228.90065], [459.60794, 230.21332], [459.90768, 231.58904], [460.13965, 233.01736], [460.30203, 234.48743], [460.39365, 235.988], [460.41376, 237.50772], [460.36224, 239.03499], [460.23944, 240.55818], [460.04636, 242.0657], [459.7844, 243.54608], [459.4556, 244.98807], [459.06244, 246.38068], [458.60794, 247.71332], [458.09552, 248.97581], [457.5291, 250.1586], [456.913, 251.25266], [456.2519, 252.24966], [455.5508, 253.14201], [454.8151, 253.92294], [454.05035, 254.58649], [453.2624, 255.1276], [452.45718, 255.54216], [451.64093, 255.82704]], [[499.0, 275.0], [498.28348, 275.15274], [497.53815, 275.19016], [496.7697, 275.11197], [495.98398, 274.9188], [495.18698, 274.61203], [494.38474, 274.1941], [493.58337, 273.66812], [492.78897, 273.03815], [492.0076, 272.3089], [491.2452, 271.48602], [490.5076, 270.5757], [489.80035, 269.5849], [489.12888, 268.52115], [488.49826, 267.39255], [487.91336, 266.20767], [487.37857, 264.9756], [486.89798, 263.7056], [486.47522, 262.40744], [486.11356, 261.09094], [485.81573, 259.76614], [485.58398, 258.4431], [485.4201, 257.13196], [485.32532, 255.8426], [485.30035, 254.5849], [485.3454, 253.36841], [485.4601, 252.2024], [485.64365, 251.0957], [485.89456, 250.0568], [486.211, 249.09357], [486.59048, 248.21333], [487.0302, 247.4228], [487.52676, 246.728], [488.07635, 246.1342], [488.67487, 245.64594], [489.31772, 245.2669], [490.0, 245.0], [490.71652, 244.84726], [491.46185, 244.80984], [492.2303, 244.88803], [493.01602, 245.08122], [493.81302, 245.38795], [494.61526, 245.80591], [495.41663, 246.33188], [496.21103, 246.96187], [496.9924, 247.69109], [497.7548, 248.51399], [498.4924, 249.4243], [499.19965, 250.4151], [499.87112, 251.47885], [500.50174, 252.60745], [501.08664, 253.79231], [501.62143, 255.02441], [502.10202, 256.2944], [502.52478, 257.59256], [502.88644, 258.90906], [503.18427, 260.23386], [503.41602, 261.5569], [503.5799, 262.86804], [503.67468, 264.1574], [503.69965, 265.4151], [503.6546, 266.6316], [503.5399, 267.7976], [503.35635, 268.9043], [503.10544, 269.9432], [502.789, 270.90643], [502.40952, 271.78665], [501.9698, 272.57718], [501.47324, 273.272], [500.92365, 273.86578], [500.32513, 274.35406]], [[338.0, 442.0], [337.20923, 441.912], [336.4283, 441.69528], [335.66315, 441.3515], [334.91956, 440.8833], [334.20325, 440.2942], [333.51962, 439.58868], [332.8739, 438.77216], [332.271, 437.8508], [331.71548, 436.83167], [331.21164, 435.72247], [330.76324, 434.53168], [330.37372, 433.26834], [330.04605, 431.94208], [329.7827, 430.563], [329.58575, 429.14154], [329.4566, 427.6886], [329.3963, 426.21518], [329.40524, 424.7325], [329.48343, 423.2519], [329.63025, 421.78455], [329.84457, 420.3417], [330.12476, 418.9343], [330.46866, 417.57306], [330.87372, 416.26834], [331.33682, 415.0301], [331.85443, 413.8677], [332.4226, 412.79004], [333.03705, 411.8053], [333.69305, 410.921], [334.38565, 410.14383], [335.10956, 409.4797], [335.85925, 408.93375], [336.62906, 408.51004], [337.41312, 408.21182], [338.20544, 408.04138], [339.0, 408.0], [339.79077, 408.088], [340.5717, 408.30472], [341.33685, 408.6485], [342.08044, 409.1167], [342.79675, 409.7058], [343.48038, 410.41132], [344.1261, 411.22784], [344.729, 412.1492], [345.28452, 413.16833], [345.78836, 414.27753], [346.23676, 415.46832], [346.62628, 416.73166], [346.95395, 418.05792], [347.2173, 419.437], [347.41425, 420.85846], [347.5434, 422.3114], [347.6037, 423.78482], [347.59476, 425.2675], [347.51657, 426.7481], [347.36975, 428.21545], [347.15543, 429.6583], [346.87524, 431.0657], [346.53134, 432.42694], [346.12628, 433.73166], [345.66318, 434.9699], [345.14557, 436.1323], [344.5774, 437.20996], [343.96295, 438.1947], [343.30695, 439.079], [342.61435, 439.85617], [341.89044, 440.5203], [341.14075, 441.06625], [340.37094, 441.48996], [339.58688, 441.78818]], [[302.0, 114.0], [302.19748, 114.743645], [302.27545, 115.52349], [302.2334, 116.3336], [302.0716, 117.16781], [301.7913, 118.01977], [301.3946, 118.882996], [300.88455, 119.75091], [300.26505, 120.61692], [299.54077, 121.47443], [298.71722, 122.31691], [297.80072, 123.137955], [296.79822, 123.93131], [295.71732, 124.69094], [294.56628, 125.41107], [293.35385, 126.086205], [292.08926, 126.71122], [290.78214, 127.28136], [289.4424, 127.79227], [288.0803, 128.24007], [286.70618, 128.62135], [285.33047, 128.93321], [283.96365, 129.1733], [282.61615, 129.33975], [281.29822, 129.4313], [280.01987, 129.4473], [278.7908, 129.38757], [277.62045, 129.25261], [276.51767, 129.04341], [275.49084, 128.76158], [274.54782, 128.40927], [273.69574, 127.98916], [272.94113, 127.50443], [272.2897, 126.958786], [271.74643, 126.35638], [271.31543, 125.70179], [271.0, 125.0], [270.80252, 124.256355], [270.72455, 123.47651], [270.7666, 122.6664], [270.9284, 121.83219], [271.2087, 120.98023], [271.6054, 120.117004], [272.11545, 119.24909], [272.73495, 118.38308], [273.45923, 117.52557], [274.28278, 116.68309], [275.19928, 115.862045], [276.20178, 115.06869], [277.28268, 114.30906], [278.43372, 113.58893], [279.64615, 112.913795], [280.91074, 112.28878], [282.21786, 111.71864], [283.5576, 111.20773], [284.9197, 110.75993], [286.29382, 110.37865], [287.66953, 110.06678], [289.03635, 109.826706], [290.38385, 109.660255], [291.70178, 109.56869], [292.98013, 109.552704], [294.2092, 109.61243], [295.37955, 109.7474], [296.48233, 109.95659], [297.50916, 110.23842], [298.45218, 110.59073], [299.30426, 111.01084], [300.05887, 111.49557], [300.7103, 112.041214], [301.25357, 112.64362]], [[326.0, 79.0], [325.50378, 79.61663], [324.90475, 80.156265], [324.20755, 80.61479], [323.41742, 80.98874], [322.5404, 81.275246], [321.5832, 81.47214], [320.553, 81.57791], [319.45773, 81.59177], [318.30573, 81.513596], [317.1057, 81.343994], [315.86685, 81.08425], [314.59854, 80.73634], [313.3105, 80.302925], [312.01242, 79.78729], [310.7143, 79.19336], [309.42593, 78.52566], [308.15717, 77.78927], [306.91763, 76.989784], [305.7168, 76.13331], [304.56378, 75.22634], [303.46735, 74.275795], [302.43585, 73.28891], [301.47717, 72.27318], [300.59854, 71.23634], [299.8067, 70.1863], [299.10767, 69.13103], [298.50674, 68.07857], [298.0085, 67.03692], [297.61676, 66.01402], [297.33447, 65.017654], [297.1638, 64.0554], [297.10605, 63.13458], [297.16162, 62.262203], [297.33014, 61.444916], [297.61032, 60.688927], [298.0, 60.0], [298.49622, 59.383373], [299.09525, 58.84374], [299.79245, 58.385204], [300.58258, 58.01126], [301.4596, 57.724754], [302.4168, 57.527866], [303.447, 57.42209], [304.54227, 57.408234], [305.69427, 57.486404], [306.8943, 57.656006], [308.13315, 57.91575], [309.40146, 58.263653], [310.6895, 58.697075], [311.98758, 59.21271], [313.2857, 59.80664], [314.57407, 60.474342], [315.84283, 61.210735], [317.08237, 62.010212], [318.2832, 62.86669], [319.43622, 63.773655], [320.53265, 64.724205], [321.56415, 65.71109], [322.52283, 66.72682], [323.40146, 67.76366], [324.1933, 68.8137], [324.89233, 69.86897], [325.49326, 70.92143], [325.9915, 71.96308], [326.38324, 72.98598], [326.66553, 73.982346], [326.8362, 74.9446], [326.89395, 75.865425], [326.83838, 76.73779], [326.66986, 77.555084]], [[215.0, 355.0], [214.21448, 354.5604], [213.4958, 353.99094], [212.84947, 353.296], [212.28038, 352.4808], [211.7929, 351.55164], [211.39069, 350.5155], [211.07683, 349.3803], [210.85371, 348.15472], [210.72305, 346.84802], [210.6858, 345.4702], [210.7423, 344.0317], [210.89207, 342.54352], [211.13399, 341.01694], [211.46622, 339.4636], [211.88623, 337.8953], [212.39081, 336.32397], [212.97615, 334.76163], [213.63777, 333.22012], [214.37064, 331.71118], [215.16919, 330.2463], [216.02733, 328.8366], [216.93854, 327.4929], [217.89587, 326.22528], [218.89207, 325.04352], [219.91953, 323.95654], [220.97041, 322.97263], [222.03676, 322.0993], [223.11043, 321.34317], [224.18326, 320.71], [225.24709, 320.20462], [226.29381, 319.83087], [227.31548, 319.59158], [228.30428, 319.4886], [229.25273, 319.52267], [230.15358, 319.69357], [231.0, 320.0], [231.78552, 320.4396], [232.5042, 321.00906], [233.15053, 321.704], [233.71962, 322.5192], [234.2071, 323.44836], [234.60931, 324.4845], [234.92317, 325.6197], [235.14629, 326.84528], [235.27695, 328.15198], [235.3142, 329.5298], [235.2577, 330.9683], [235.10793, 332.45648], [234.86601, 333.98306], [234.53378, 335.5364], [234.11377, 337.1047], [233.60919, 338.67603], [233.02385, 340.23837], [232.36223, 341.77988], [231.62936, 343.28882], [230.83081, 344.7537], [229.97267, 346.1634], [229.06146, 347.5071], [228.10413, 348.77472], [227.10793, 349.95648], [226.08047, 351.04346], [225.02959, 352.02737], [223.96324, 352.9007], [222.88957, 353.65683], [221.81674, 354.29], [220.75291, 354.79538], [219.70619, 355.16913], [218.68452, 355.40842], [217.69572, 355.5114], [216.74727, 355.47733]], [[345.0, 508.0], [344.26157, 507.84586], [343.544, 507.57114], [342.85272, 507.1779], [342.193, 506.66913], [341.5699, 506.04874], [340.98807, 505.3214], [340.45203, 504.49274], [339.96582, 503.56894], [339.5331, 502.55713], [339.15726, 501.46497], [338.8411, 500.30075], [338.58704, 499.07336], [338.397, 497.79218], [338.27243, 496.4669], [338.21426, 495.1076], [338.223, 493.72467], [338.2985, 492.32858], [338.44025, 490.93002], [338.64713, 489.5396], [338.91757, 488.1679], [339.24954, 486.82538], [339.6405, 485.52225], [340.08746, 484.26837], [340.58704, 483.07336], [341.1354, 481.94632], [341.72842, 480.89575], [342.36154, 479.92972], [343.02997, 479.0555], [343.72864, 478.27985], [344.45218, 477.6086], [345.1951, 477.04688], [345.95178, 476.59897], [346.71643, 476.26825], [347.48322, 476.05728], [348.24637, 475.96762], [349.0, 476.0], [349.73843, 476.15414], [350.456, 476.42886], [351.14728, 476.8221], [351.807, 477.33087], [352.4301, 477.95126], [353.01193, 478.6786], [353.54797, 479.50726], [354.03418, 480.43106], [354.4669, 481.44287], [354.84274, 482.53503], [355.1589, 483.69925], [355.41296, 484.92664], [355.603, 486.20782], [355.72757, 487.5331], [355.78574, 488.8924], [355.777, 490.27533], [355.7015, 491.67142], [355.55975, 493.06998], [355.35287, 494.4604], [355.08243, 495.8321], [354.75046, 497.17462], [354.3595, 498.47775], [353.91254, 499.73163], [353.41296, 500.92664], [352.8646, 502.05368], [352.27158, 503.10425], [351.63846, 504.07028], [350.97003, 504.9445], [350.27136, 505.72015], [349.54782, 506.3914], [348.8049, 506.95312], [348.04822, 507.40103], [347.28357, 507.73175], [346.51678, 507.94272]], [[160.0, 210.0], [160.21127, 210.86212], [160.28394, 211.76334], [160.21745, 212.69681], [160.01231, 213.65541], [159.6701, 214.63185], [159.1934, 215.61871], [158.58586, 216.60847], [157.8521, 217.5936], [156.99767, 218.5666], [156.02911, 219.52008], [154.95378, 220.44675], [153.77986, 221.33958], [152.5163, 222.19179], [151.17268, 222.99686], [149.75928, 223.74869], [148.2868, 224.44154], [146.7665, 225.07014], [145.20992, 225.62973], [143.62889, 226.11601], [142.03548, 226.52531], [140.44179, 226.85452], [138.85997, 227.1011], [137.30203, 227.2632], [135.77986, 227.33958], [134.30502, 227.32967], [132.88876, 227.23352], [131.54182, 227.05188], [130.27449, 226.78613], [129.09639, 226.4383], [128.0165, 226.01102], [127.04303, 225.50754], [126.18339, 224.93172], [125.44412, 224.28792], [124.83086, 223.58102], [124.34826, 222.81645], [124.0, 222.0], [123.788734, 221.13788], [123.716064, 220.23666], [123.78255, 219.30319], [123.98768, 218.34459], [124.329895, 217.36815], [124.80659, 216.38129], [125.41414, 215.39153], [126.14791, 214.4064], [127.00233, 213.4334], [127.970894, 212.47992], [129.04622, 211.55325], [130.22014, 210.66042], [131.4837, 209.80821], [132.82732, 209.00314], [134.24072, 208.25131], [135.7132, 207.55846], [137.2335, 206.92986], [138.79008, 206.37027], [140.37111, 205.88399], [141.96452, 205.47469], [143.55821, 205.14548], [145.14003, 204.8989], [146.69797, 204.7368], [148.22014, 204.66042], [149.69498, 204.67033], [151.11124, 204.76648], [152.45818, 204.94812], [153.72551, 205.21387], [154.90361, 205.5617], [155.9835, 205.98898], [156.95697, 206.49246], [157.81662, 207.06828], [158.55588, 207.71208], [159.16914, 208.41898]], [[421.0, 481.0], [420.68216, 480.29346], [420.47708, 479.53143], [420.38635, 478.71967], [420.41064, 477.86438], [420.54977, 476.9721], [420.8027, 476.04956], [421.16745, 475.10382], [421.6413, 474.14206], [422.2206, 473.17163], [422.901, 472.1999], [423.67728, 471.23422], [424.54352, 470.282], [425.49313, 469.35046], [426.51895, 468.44672], [427.61307, 467.5776], [428.76724, 466.74976], [429.97266, 465.96948], [431.22012, 465.2427], [432.50015, 464.57498], [433.80304, 463.97137], [435.11884, 463.4365], [436.43753, 462.97437], [437.74908, 462.58856], [439.04352, 462.282], [440.31097, 462.057], [441.54184, 461.91528], [442.7267, 461.8579], [443.8566, 461.88538], [444.92288, 461.99738], [445.91742, 462.19315], [446.8327, 462.47116], [447.66174, 462.8293], [448.39822, 463.26486], [449.03653, 463.7745], [449.5718, 464.35434], [450.0, 465.0], [450.31784, 465.70654], [450.52292, 466.46857], [450.61365, 467.28033], [450.58936, 468.13562], [450.45023, 469.0279], [450.1973, 469.95044], [449.83255, 470.89618], [449.3587, 471.85794], [448.7794, 472.82837], [448.099, 473.8001], [447.32272, 474.76578], [446.45648, 475.718], [445.50687, 476.64954], [444.48105, 477.55328], [443.38693, 478.4224], [442.23276, 479.25024], [441.02734, 480.03052], [439.77988, 480.7573], [438.49985, 481.42502], [437.19696, 482.02863], [435.88116, 482.5635], [434.56247, 483.02563], [433.25092, 483.41144], [431.95648, 483.718], [430.68903, 483.943], [429.45816, 484.08472], [428.2733, 484.1421], [427.1434, 484.11462], [426.07712, 484.00262], [425.08258, 483.80685], [424.1673, 483.52884], [423.33826, 483.1707], [422.60178, 482.73514], [421.96347, 482.2255]], [[480.0, 83.0], [479.70978, 83.68038], [479.30762, 84.31753], [478.79657, 84.90659], [478.1805, 85.4431], [477.46414, 85.92295], [476.65292, 86.342514], [475.75302, 86.69858], [474.77127, 86.98845], [473.71515, 87.20991], [472.5927, 87.36127], [471.41248, 87.44139], [470.18344, 87.44965], [468.91498, 87.386], [467.6167, 87.25092], [466.2985, 87.04543], [464.97043, 86.771095], [463.64258, 86.430016], [462.32507, 86.02477], [461.02792, 85.55846], [459.761, 85.034615], [458.53394, 84.45724], [457.35608, 83.83072], [456.23642, 83.15982], [455.18344, 82.44965], [454.20517, 81.70563], [453.30908, 80.933395], [452.50195, 80.13884], [451.78992, 79.328], [451.17844, 78.50706], [450.67215, 77.68226], [450.2749, 76.85988], [449.98975, 76.04617], [449.8188, 75.24734], [449.7634, 74.46945], [449.82394, 73.71843], [450.0, 73.0], [450.29022, 72.31962], [450.69238, 71.68247], [451.20343, 71.09341], [451.8195, 70.5569], [452.53586, 70.07705], [453.34708, 69.657486], [454.24698, 69.30142], [455.22873, 69.01155], [456.28485, 68.79009], [457.4073, 68.63873], [458.58752, 68.55861], [459.81656, 68.55035], [461.08502, 68.614], [462.3833, 68.74908], [463.7015, 68.95457], [465.02957, 69.228905], [466.35742, 69.569984], [467.67493, 69.97523], [468.97208, 70.44154], [470.239, 70.965385], [471.46606, 71.54276], [472.64392, 72.16928], [473.76358, 72.84018], [474.81656, 73.55035], [475.79483, 74.29437], [476.69092, 75.066605], [477.49805, 75.86116], [478.21008, 76.672], [478.82156, 77.49294], [479.32785, 78.31774], [479.7251, 79.14012], [480.01025, 79.95383], [480.1812, 80.75266], [480.2366, 81.53055]], [[67.0, 177.0], [66.28489, 176.84776], [65.59045, 176.57872], [64.92195, 176.19492], [64.28449, 175.69928], [63.682922, 175.09558], [63.12182, 174.38841], [62.60545, 173.58315], [62.137745, 172.68593], [61.72227, 171.70357], [61.362183, 170.64357], [61.060223, 169.51396], [60.818687, 168.32338], [60.63942, 167.08086], [60.52378, 165.79587], [60.47265, 164.47818], [60.486416, 163.13783], [60.56498, 161.78502], [60.707737, 160.43002], [60.9136, 159.08319], [61.18101, 157.75475], [61.507927, 156.45479], [61.89186, 155.19325], [62.32989, 153.97969], [62.818687, 152.82338], [63.354527, 151.7331], [63.93333, 150.71715], [64.5507, 149.78326], [65.20193, 148.93855], [65.88206, 148.18944], [66.585915, 147.54163], [67.30815, 147.00005], [68.04327, 146.56882], [68.78565, 146.25122], [69.52968, 146.04968], [70.26967, 145.96573], [71.0, 146.0], [71.71511, 146.15224], [72.40955, 146.42128], [73.07805, 146.80508], [73.71551, 147.30072], [74.31708, 147.90442], [74.87818, 148.61159], [75.394554, 149.41685], [75.86225, 150.31407], [76.27773, 151.29643], [76.63782, 152.35643], [76.93978, 153.48604], [77.18131, 154.67662], [77.36058, 155.91914], [77.47622, 157.20413], [77.52735, 158.52182], [77.51358, 159.86217], [77.43502, 161.21498], [77.29227, 162.56998], [77.086395, 163.91681], [76.81899, 165.24525], [76.49207, 166.54521], [76.10814, 167.80675], [75.670105, 169.02031], [75.18131, 170.17662], [74.64547, 171.2669], [74.066666, 172.28285], [73.4493, 173.21674], [72.79807, 174.06145], [72.11794, 174.81056], [71.414085, 175.45837], [70.69185, 175.99995], [69.95673, 176.43118], [69.21435, 176.74878], [68.47032, 176.95032]], [[198.0, 359.0], [197.38434, 359.0671], [196.75433, 359.03476], [196.11478, 358.9032], [195.47055, 358.67343], [194.82654, 358.34723], [194.18767, 357.92706], [193.55876, 357.4161], [192.94466, 356.81827], [192.34998, 356.13812], [191.77928, 355.3808], [191.2369, 354.5521], [190.72696, 353.65826], [190.25336, 352.70618], [189.81967, 351.70306], [189.42923, 350.65652], [189.08498, 349.57455], [188.78955, 348.4654], [188.5452, 347.33746], [188.35378, 346.19934], [188.21674, 345.05972], [188.13513, 343.92725], [188.10957, 342.81055], [188.14026, 341.7181], [188.22696, 340.65826], [188.36902, 339.6391], [188.56534, 338.6683], [188.81444, 337.75333], [189.11443, 336.90112], [189.46301, 336.11816], [189.85753, 335.4104], [190.295, 334.78323], [190.77208, 334.24142], [191.28516, 333.78912], [191.83029, 333.42975], [192.40337, 333.16605], [193.0, 333.0], [193.61566, 332.9329], [194.24567, 332.96524], [194.88522, 333.0968], [195.52945, 333.32657], [196.17346, 333.65277], [196.81233, 334.07294], [197.44124, 334.5839], [198.05534, 335.18173], [198.65002, 335.86188], [199.22072, 336.6192], [199.7631, 337.4479], [200.27304, 338.34174], [200.74664, 339.29382], [201.18033, 340.29694], [201.57077, 341.34348], [201.91502, 342.42545], [202.21045, 343.5346], [202.4548, 344.66254], [202.64622, 345.80066], [202.78326, 346.94028], [202.86487, 348.07275], [202.89043, 349.18945], [202.85974, 350.2819], [202.77304, 351.34174], [202.63098, 352.3609], [202.43466, 353.3317], [202.18556, 354.24667], [201.88557, 355.09888], [201.53699, 355.88184], [201.14247, 356.5896], [200.705, 357.21677], [200.22792, 357.75858], [199.71484, 358.21088], [199.16971, 358.57025]], [[298.0, 158.0], [297.39862, 158.46724], [296.7181, 158.83958], [295.96362, 159.11421], [295.1409, 159.28905], [294.25626, 159.36273], [293.31638, 159.33472], [292.32843, 159.20523], [291.2999, 158.97523], [290.23868, 158.64648], [289.1528, 158.2215], [288.05054, 157.70349], [286.94028, 157.09642], [285.83047, 156.40489], [284.72955, 155.63417], [283.64594, 154.79015], [282.58783, 153.87921], [281.56332, 152.90831], [280.58017, 151.88483], [279.6459, 150.81657], [278.76758, 149.71165], [277.95193, 148.57849], [277.2051, 147.42569], [276.53287, 146.26205], [275.94028, 145.09642], [275.43185, 143.93765], [275.01147, 142.79459], [274.68234, 141.67592], [274.44693, 140.59016], [274.30707, 139.54558], [274.26382, 138.55011], [274.31747, 137.61134], [274.46768, 136.73642], [274.71326, 135.93199], [275.05234, 135.2042], [275.48233, 134.55856], [276.0, 134.0], [276.60138, 133.53276], [277.2819, 133.16042], [278.03638, 132.88579], [278.8591, 132.71095], [279.74374, 132.63727], [280.68362, 132.66528], [281.67157, 132.79477], [282.7001, 133.02477], [283.76132, 133.35352], [284.8472, 133.7785], [285.94946, 134.29651], [287.05972, 134.90358], [288.16953, 135.59511], [289.27045, 136.36583], [290.35406, 137.20985], [291.41217, 138.12079], [292.43668, 139.09169], [293.41983, 140.11517], [294.3541, 141.18343], [295.23242, 142.28835], [296.04807, 143.42151], [296.7949, 144.57431], [297.46713, 145.73795], [298.05972, 146.90358], [298.56815, 148.06235], [298.98853, 149.20541], [299.31766, 150.32408], [299.55307, 151.40984], [299.69293, 152.45442], [299.73618, 153.44989], [299.68253, 154.38866], [299.53232, 155.26358], [299.28674, 156.06801], [298.94766, 156.7958]], [[-1.0, 274.0], [-0.66125584, 273.3001], [-0.2071253, 272.6512], [0.35893536, 272.05826], [1.032618, 271.52573], [1.8087955, 271.05768], [2.6815605, 270.6577], [3.6442714, 270.32883], [4.689601, 270.07358], [5.809593, 269.89383], [6.9957256, 269.79105], [8.238969, 269.76593], [9.529862, 269.8187], [10.8585825, 269.94894], [12.215015, 270.1557], [13.588839, 270.43738], [14.969597, 270.7918], [16.34678, 271.21637], [17.70991, 271.70773], [19.048609, 272.26224], [20.35269, 272.8756], [21.612228, 273.5432], [22.81764, 274.25995], [23.95975, 275.02036], [25.029863, 275.8187], [26.019838, 276.64883], [26.92214, 277.5045], [27.729904, 278.37912], [28.43698, 279.2661], [29.037985, 280.15866], [29.528349, 281.05002], [29.904337, 281.93338], [30.16309, 282.80203], [30.302635, 283.64935], [30.321915, 284.4689], [30.22078, 285.25446], [30.0, 286.0], [29.661255, 286.6999], [29.207125, 287.3488], [28.641066, 287.94174], [27.967382, 288.47427], [27.191204, 288.94232], [26.31844, 289.3423], [25.355728, 289.67117], [24.3104, 289.92642], [23.190407, 290.10617], [22.004274, 290.20895], [20.761032, 290.23407], [19.470137, 290.1813], [18.141418, 290.05106], [16.784985, 289.8443], [15.411161, 289.56262], [14.030403, 289.2082], [12.65322, 288.78363], [11.2900915, 288.29227], [9.951392, 287.73776], [8.64731, 287.1244], [7.3877707, 286.4568], [6.1823597, 285.74005], [5.0402513, 284.97964], [3.9701376, 284.1813], [2.9801617, 283.35117], [2.077859, 282.4955], [1.2700965, 281.62088], [0.5630214, 280.7339], [-0.037984848, 279.84134], [-0.5283482, 278.94998], [-0.9043369, 278.06662], [-1.1630892, 277.19797], [-1.3026359, 276.35065], [-1.321915, 275.5311]], [[274.0, 487.0], [273.22876, 486.96054], [272.45956, 486.79578], [271.6983, 486.50702], [270.95074, 486.0964], [270.2226, 485.56714], [269.51938, 484.92316], [268.84647, 484.16943], [268.20898, 483.31168], [267.61176, 482.3564], [267.05933, 481.3109], [266.55594, 480.18314], [266.10538, 478.98166], [265.7111, 477.71564], [265.3761, 476.39468], [265.10294, 475.0289], [264.89368, 473.62863], [264.7499, 472.20456], [264.67276, 470.7675], [264.66275, 469.3284], [264.72003, 467.89822], [264.84412, 466.48785], [265.0341, 465.10803], [265.28848, 463.76923], [265.60538, 462.48166], [265.98236, 461.2551], [266.41653, 460.0989], [266.90463, 459.02188], [267.44293, 458.0322], [268.0273, 457.13742], [268.65335, 456.34433], [269.31628, 455.65897], [270.01105, 455.08655], [270.7324, 454.63147], [271.47476, 454.29712], [272.23254, 454.0861], [273.0, 454.0], [273.77124, 454.03946], [274.54044, 454.20422], [275.3017, 454.49298], [276.04926, 454.9036], [276.7774, 455.43286], [277.48062, 456.07684], [278.15353, 456.83057], [278.79102, 457.68832], [279.38824, 458.6436], [279.94067, 459.6891], [280.44406, 460.81686], [280.89462, 462.01834], [281.2889, 463.28436], [281.6239, 464.60532], [281.89706, 465.9711], [282.10632, 467.37137], [282.2501, 468.79544], [282.32724, 470.2325], [282.33725, 471.6716], [282.27997, 473.10178], [282.15588, 474.51215], [281.9659, 475.89197], [281.71152, 477.23077], [281.39462, 478.51834], [281.01764, 479.7449], [280.58347, 480.9011], [280.09537, 481.97812], [279.55707, 482.9678], [278.9727, 483.86258], [278.34665, 484.65567], [277.68372, 485.34103], [276.98895, 485.91345], [276.2676, 486.36853], [275.52524, 486.70288]], [[274.0, 257.0], [273.30728, 257.4363], [272.5399, 257.76276], [271.70374, 257.97684], [270.8051, 258.07693], [269.85092, 258.0623], [268.84836, 257.933], [267.80515, 257.6901], [266.72913, 257.3354], [265.62854, 256.87155], [264.51175, 256.30215], [263.38727, 255.63153], [262.26364, 254.86476], [261.14944, 254.00769], [260.0531, 253.06685], [258.98303, 252.04941], [257.9473, 250.96307], [256.95386, 249.81615], [256.01022, 248.61734], [255.12358, 247.37578], [254.30069, 246.10092], [253.54782, 244.80247], [252.8707, 243.4903], [252.27446, 242.17438], [251.76366, 240.86476], [251.34218, 239.57138], [251.01321, 238.30411], [250.7793, 237.07257], [250.6422, 235.88614], [250.60294, 234.75385], [250.66183, 233.68431], [250.81845, 232.68568], [251.07156, 231.76555], [251.41928, 230.93091], [251.85893, 230.18813], [252.38718, 229.54286], [253.0, 229.0], [253.69273, 228.56369], [254.46011, 228.23724], [255.29628, 228.02316], [256.1949, 227.92307], [257.14908, 227.9377], [258.15164, 228.06697], [259.19485, 228.30989], [260.27087, 228.66463], [261.37146, 229.12845], [262.48825, 229.69785], [263.61273, 230.36847], [264.73636, 231.13524], [265.85056, 231.99231], [266.9469, 232.93315], [268.01697, 233.95059], [269.0527, 235.03693], [270.04614, 236.18385], [270.98978, 237.38266], [271.87643, 238.62422], [272.6993, 239.89908], [273.45218, 241.19753], [274.1293, 242.5097], [274.72556, 243.82562], [275.23636, 245.13524], [275.65784, 246.42862], [275.9868, 247.69589], [276.2207, 248.92743], [276.35782, 250.11386], [276.39706, 251.24615], [276.33817, 252.31569], [276.18155, 253.31432], [275.92844, 254.23445], [275.58072, 255.06909], [275.14108, 255.81187]], [[171.0, 298.0], [170.49806, 298.68655], [169.88197, 299.29562], [169.15643, 299.82248], [168.32697, 300.2632], [167.39987, 300.61438], [166.38222, 300.87338], [165.28174, 301.03818], [164.10681, 301.10757], [162.8664, 301.08102], [161.5699, 300.95874], [160.22722, 300.7416], [158.84856, 300.4313], [157.4444, 300.0302], [156.02547, 299.54138], [154.60252, 298.9685], [153.1864, 298.31595], [151.7879, 297.58868], [150.41765, 296.79227], [149.08607, 295.93274], [147.80331, 295.01663], [146.57913, 294.05093], [145.42284, 293.043], [144.34323, 292.00046], [143.34856, 290.9313], [142.44635, 289.84366], [141.64348, 288.74576], [140.94608, 287.646], [140.35944, 286.55273], [139.88803, 285.4743], [139.53543, 284.41888], [139.30434, 283.39453], [139.1965, 282.40903], [139.21274, 281.4699], [139.35294, 280.58426], [139.61603, 279.75888], [140.0, 279.0], [140.50194, 278.31345], [141.11803, 277.70438], [141.84357, 277.17752], [142.67303, 276.7368], [143.60013, 276.38562], [144.61778, 276.12662], [145.71826, 275.96182], [146.89319, 275.89243], [148.1336, 275.91898], [149.4301, 276.04126], [150.77278, 276.2584], [152.15144, 276.5687], [153.5556, 276.9698], [154.97453, 277.45862], [156.39748, 278.0315], [157.8136, 278.68405], [159.2121, 279.41132], [160.58235, 280.20773], [161.91393, 281.06726], [163.19669, 281.98337], [164.42087, 282.94907], [165.57716, 283.957], [166.65677, 284.99954], [167.65144, 286.0687], [168.55365, 287.15634], [169.35652, 288.25424], [170.05392, 289.354], [170.64056, 290.44727], [171.11197, 291.5257], [171.46457, 292.58112], [171.69566, 293.60547], [171.8035, 294.59097], [171.78726, 295.5301], [171.64706, 296.41574]], [[256.0, 499.0], [255.43382, 499.61093], [254.76541, 500.13348], [253.99985, 500.5637], [253.14296, 500.8983], [252.20128, 501.1347], [251.18195, 501.27118], [250.09273, 501.30664], [248.94193, 501.24084], [247.7383, 501.07425], [246.49098, 500.80817], [245.2095, 500.4446], [243.90358, 499.98636], [242.58319, 499.43686], [241.25835, 498.80032], [239.93915, 498.0816], [238.63565, 497.28613], [237.35774, 496.42], [236.11517, 495.48978], [234.91737, 494.50256], [233.7735, 493.46588], [232.69221, 492.38757], [231.68178, 491.27588], [230.74988, 490.13925], [229.90358, 488.98636], [229.14935, 487.82593], [228.49294, 486.66684], [227.9393, 485.5179], [227.49268, 484.38785], [227.15646, 483.28528], [226.93323, 482.21863], [226.82465, 481.19592], [226.83157, 480.22504], [226.95393, 479.31332], [227.1908, 478.4677], [227.54037, 477.69464], [228.0, 477.0], [228.56618, 476.38907], [229.23459, 475.86652], [230.00015, 475.4363], [230.85704, 475.1017], [231.79872, 474.8653], [232.81805, 474.72882], [233.90727, 474.69336], [235.05807, 474.75916], [236.2617, 474.92575], [237.50902, 475.19183], [238.7905, 475.5554], [240.09642, 476.01364], [241.41681, 476.56314], [242.74165, 477.19968], [244.06085, 477.9184], [245.36435, 478.71387], [246.64226, 479.58], [247.88483, 480.51022], [249.08263, 481.49744], [250.2265, 482.53412], [251.30779, 483.61243], [252.31822, 484.72412], [253.25012, 485.86075], [254.09642, 487.01364], [254.85065, 488.17407], [255.50706, 489.33316], [256.0607, 490.4821], [256.50732, 491.61215], [256.84354, 492.71472], [257.06677, 493.78137], [257.17535, 494.80408], [257.16843, 495.77496], [257.04608, 496.68668], [256.8092, 497.5323]], [[140.0, 157.0], [139.2768, 156.65935], [138.60477, 156.19952], [137.98901, 155.62402], [137.43423, 154.93723], [136.94464, 154.14436], [136.52397, 153.25145], [136.17542, 152.2653], [135.90163, 151.19342], [135.7047, 150.04396], [135.58612, 148.82567], [135.5468, 147.54782], [135.58704, 146.22014], [135.70651, 144.85272], [135.90434, 143.456], [136.179, 142.04057], [136.5284, 140.61723], [136.94987, 139.1968], [137.44025, 137.79008], [137.99574, 136.4078], [138.61217, 135.06049], [139.28482, 133.75836], [140.00858, 132.51135], [140.77794, 131.32895], [141.58704, 130.22014], [142.42972, 129.19337], [143.29958, 128.25647], [144.18997, 127.41654], [145.09416, 126.68], [146.00523, 126.05244], [146.91628, 125.538635], [147.82034, 125.14251], [148.71054, 124.867065], [149.58012, 124.7144], [150.42245, 124.685684], [151.23114, 124.78112], [152.0, 125.0], [152.7232, 125.340645], [153.39523, 125.80047], [154.01099, 126.37597], [154.56577, 127.062775], [155.05536, 127.855644], [155.47603, 128.74855], [155.82458, 129.7347], [156.09837, 130.80658], [156.2953, 131.95604], [156.41388, 133.17433], [156.4532, 134.45218], [156.41296, 135.77986], [156.29349, 137.14728], [156.09566, 138.544], [155.821, 139.95943], [155.4716, 141.38277], [155.05013, 142.8032], [154.55975, 144.20992], [154.00426, 145.5922], [153.38783, 146.93951], [152.71518, 148.24164], [151.99142, 149.48865], [151.22206, 150.67105], [150.41296, 151.77986], [149.57028, 152.80663], [148.70042, 153.74353], [147.81003, 154.58345], [146.90584, 155.32], [145.99477, 155.94756], [145.08372, 156.46136], [144.17966, 156.8575], [143.28946, 157.13293], [142.41988, 157.2856], [141.57755, 157.31432]], [[200.0, 412.0], [200.7389, 412.4358], [201.4113, 412.99387], [202.01207, 413.66995], [202.53664, 414.45886], [202.98102, 415.35468], [203.34183, 416.35052], [203.61632, 417.43884], [203.8024, 418.61133], [203.89867, 419.85907], [203.90437, 421.17258], [203.81947, 422.54187], [203.64462, 423.95648], [203.38115, 425.4057], [203.03107, 426.87845], [202.59702, 428.36353], [202.08234, 429.84967], [201.4909, 431.32553], [200.82726, 432.77988], [200.09642, 434.20166], [199.30396, 435.58005], [198.45592, 436.90457], [197.55875, 438.1651], [196.61926, 439.35208], [195.64462, 440.45648], [194.64224, 441.46988], [193.61977, 442.38458], [192.58496, 443.1936], [191.54568, 443.89078], [190.50989, 444.47086], [189.48543, 444.92935], [188.48009, 445.26285], [187.50156, 445.46872], [186.55725, 445.5455], [185.65437, 445.49252], [184.79979, 445.31024], [184.0, 445.0], [183.2611, 444.5642], [182.5887, 444.00613], [181.98793, 443.33005], [181.46336, 442.54114], [181.01898, 441.64532], [180.65817, 440.64948], [180.38368, 439.56116], [180.1976, 438.38867], [180.10133, 437.14093], [180.09563, 435.82742], [180.18053, 434.45813], [180.35538, 433.04352], [180.61885, 431.5943], [180.96893, 430.12155], [181.40298, 428.63647], [181.91766, 427.15033], [182.5091, 425.67447], [183.17274, 424.22012], [183.90358, 422.79834], [184.69604, 421.41995], [185.54408, 420.09543], [186.44125, 418.8349], [187.38074, 417.64792], [188.35538, 416.54352], [189.35776, 415.53012], [190.38023, 414.61542], [191.41504, 413.8064], [192.45432, 413.10922], [193.49011, 412.52914], [194.51457, 412.07065], [195.51991, 411.73715], [196.49844, 411.53128], [197.44275, 411.4545], [198.34563, 411.50748]], [[393.0, 334.0], [392.3734, 334.48865], [391.66406, 334.87845], [390.87735, 335.1664], [390.01926, 335.35037], [389.09637, 335.42892], [388.11563, 335.4015], [387.08456, 335.26822], [386.011, 335.03018], [384.90308, 334.68918], [383.7693, 334.2478], [382.6182, 333.70938], [381.45862, 333.07806], [380.29935, 332.35864], [379.1492, 331.55655], [378.01697, 330.67792], [376.91122, 329.72946], [375.84042, 328.71835], [374.81268, 327.6523], [373.83585, 326.53946], [372.91733, 325.38824], [372.06415, 324.20746], [371.28275, 323.00604], [370.57913, 321.79318], [369.95862, 320.57806], [369.42593, 319.37], [368.98517, 318.1781], [368.63962, 317.0115], [368.39197, 315.8791], [368.24408, 314.78943], [368.19705, 313.75085], [368.25128, 312.77124], [368.40634, 311.85806], [368.66104, 311.01828], [369.01346, 310.25824], [369.46094, 309.58377], [370.0, 309.0], [370.6266, 308.51135], [371.33594, 308.12155], [372.12265, 307.8336], [372.98074, 307.64963], [373.90363, 307.57108], [374.88437, 307.5985], [375.91544, 307.73178], [376.989, 307.96982], [378.09692, 308.31082], [379.2307, 308.7522], [380.3818, 309.29062], [381.54138, 309.92194], [382.70065, 310.64136], [383.8508, 311.44345], [384.98303, 312.32208], [386.08878, 313.27054], [387.15958, 314.28165], [388.18732, 315.3477], [389.16415, 316.46054], [390.08267, 317.61176], [390.93585, 318.79254], [391.71725, 319.99396], [392.42087, 321.20682], [393.04138, 322.42194], [393.57407, 323.63], [394.01483, 324.8219], [394.36038, 325.9885], [394.60803, 327.1209], [394.75592, 328.21057], [394.80295, 329.24915], [394.74872, 330.22876], [394.59366, 331.14194], [394.33896, 331.98172], [393.98654, 332.74176]], [[242.0, 345.0], [241.44142, 344.3677], [240.98221, 343.64127], [240.62589, 342.82623], [240.37515, 341.9288], [240.23193, 340.9558], [240.19728, 339.91467], [240.27148, 338.8133], [240.45398, 337.66006], [240.74338, 336.46375], [241.13747, 335.23346], [241.63326, 333.97858], [242.22696, 332.70862], [242.91408, 331.4333], [243.68938, 330.16223], [244.54694, 328.9052], [245.48026, 327.67172], [246.48221, 326.47116], [247.5452, 325.31268], [248.6611, 324.2051], [249.82146, 323.15686], [251.01741, 322.1759], [252.23987, 321.2697], [253.47954, 320.4452], [254.72696, 319.70862], [255.97267, 319.06558], [257.20715, 318.521], [258.42105, 318.07898], [259.6051, 317.7429], [260.75027, 317.51535], [261.84793, 317.398], [262.88962, 317.3918], [263.86746, 317.4968], [264.77405, 317.71216], [265.60242, 318.03625], [266.34628, 318.46664], [267.0, 319.0], [267.5586, 319.6323], [268.0178, 320.35873], [268.3741, 321.17377], [268.62485, 322.0712], [268.76807, 323.0442], [268.8027, 324.08533], [268.72852, 325.1867], [268.54602, 326.33994], [268.25662, 327.53625], [267.86252, 328.76654], [267.36676, 330.02142], [266.77304, 331.29138], [266.0859, 332.5667], [265.31064, 333.83777], [264.45306, 335.0948], [263.51974, 336.32828], [262.5178, 337.52884], [261.4548, 338.68732], [260.3389, 339.7949], [259.17853, 340.84314], [257.98257, 341.8241], [256.76013, 342.7303], [255.52046, 343.5548], [254.27304, 344.29138], [253.02733, 344.93442], [251.79285, 345.479], [250.57895, 345.92102], [249.39491, 346.2571], [248.24971, 346.48465], [247.15208, 346.602], [246.11038, 346.6082], [245.13252, 346.5032], [244.22597, 346.28784], [243.3976, 345.96375]], [[189.0, 355.0], [188.7331, 355.68228], [188.35406, 356.32513], [187.8658, 356.92365], [187.272, 357.47324], [186.5772, 357.9698], [185.78667, 358.40952], [184.90643, 358.789], [183.9432, 359.10544], [182.9043, 359.35635], [181.7976, 359.5399], [180.63159, 359.6546], [179.4151, 359.69965], [178.1574, 359.67468], [176.86806, 359.5799], [175.55688, 359.41602], [174.23387, 359.18427], [172.90907, 358.88644], [171.59258, 358.52478], [170.29439, 358.10202], [169.02441, 357.62143], [167.79231, 357.08664], [166.60745, 356.50174], [165.47885, 355.87112], [164.4151, 355.19965], [163.4243, 354.4924], [162.51399, 353.7548], [161.69109, 352.9924], [160.96187, 352.21103], [160.33188, 351.41663], [159.80591, 350.61526], [159.38795, 349.81302], [159.08122, 349.01602], [158.88803, 348.2303], [158.80984, 347.46185], [158.84726, 346.71652], [159.0, 346.0], [159.2669, 345.31772], [159.64594, 344.67487], [160.1342, 344.07635], [160.728, 343.52676], [161.4228, 343.0302], [162.21333, 342.59048], [163.09357, 342.211], [164.0568, 341.89456], [165.0957, 341.64365], [166.2024, 341.4601], [167.36841, 341.3454], [168.5849, 341.30035], [169.8426, 341.32532], [171.13194, 341.4201], [172.44312, 341.58398], [173.76613, 341.81573], [175.09093, 342.11356], [176.40742, 342.47522], [177.70561, 342.89798], [178.97559, 343.37857], [180.20769, 343.91336], [181.39255, 344.49826], [182.52115, 345.12888], [183.5849, 345.80035], [184.5757, 346.5076], [185.48601, 347.2452], [186.30891, 348.0076], [187.03813, 348.78897], [187.66812, 349.58337], [188.19409, 350.38474], [188.61205, 351.18698], [188.91878, 351.98398], [189.11197, 352.7697], [189.19016, 353.53815]], [[382.0, 401.0], [381.22495, 401.00714], [380.44437, 400.88867], [379.6642, 400.64548], [378.89044, 400.2794], [378.1289, 399.7932], [377.3854, 399.19064], [376.66562, 398.4763], [375.975, 397.65555], [375.31885, 396.7347], [374.70212, 395.72073], [374.12952, 394.62137], [373.60538, 393.44498], [373.13373, 392.2005], [372.71814, 390.8974], [372.36176, 389.54565], [372.06732, 388.1555], [371.83707, 386.7375], [371.67276, 385.3025], [371.5756, 383.86136], [371.5464, 382.42508], [371.5853, 381.0046], [371.69208, 379.61075], [371.86588, 378.2541], [372.10538, 376.94498], [372.40878, 375.69333], [372.77374, 374.50873], [373.19754, 373.40018], [373.67688, 372.3761], [374.20816, 371.44427], [374.78732, 370.61182], [375.40997, 369.88507], [376.07135, 369.26953], [376.76645, 368.76993], [377.48996, 368.39], [378.23636, 368.13272], [379.0, 368.0], [379.77505, 367.99286], [380.55563, 368.11133], [381.3358, 368.35452], [382.10956, 368.7206], [382.8711, 369.2068], [383.6146, 369.80936], [384.33438, 370.5237], [385.025, 371.34445], [385.68115, 372.2653], [386.29788, 373.27927], [386.87048, 374.37863], [387.39462, 375.55502], [387.86627, 376.7995], [388.28186, 378.1026], [388.63824, 379.45435], [388.93268, 380.8445], [389.16293, 382.2625], [389.32724, 383.6975], [389.4244, 385.13864], [389.4536, 386.57492], [389.4147, 387.9954], [389.30792, 389.38925], [389.13412, 390.7459], [388.89462, 392.05502], [388.59122, 393.30667], [388.22626, 394.49127], [387.80246, 395.59982], [387.32312, 396.6239], [386.79184, 397.55573], [386.21268, 398.38818], [385.59003, 399.11493], [384.92865, 399.73047], [384.23355, 400.23007], [383.51004, 400.61]], [[63.0, 140.0], [63.174152, 140.74174], [63.229015, 141.5159], [63.164173, 142.31657], [62.980114, 143.13766], [62.67824, 143.97293], [62.260857, 144.81601], [61.73113, 145.66049], [61.093098, 146.49994], [60.351612, 147.32799], [59.512318, 148.1383], [58.581604, 148.92474], [57.56655, 149.6813], [56.474888, 150.40225], [55.31492, 151.08208], [54.095474, 151.71562], [52.825832, 152.29805], [51.51566, 152.82494], [50.174923, 153.29227], [48.81383, 153.69649], [47.442738, 154.03453], [46.072083, 154.3038], [44.712296, 154.50227], [43.373722, 154.62843], [42.06655, 154.6813], [40.800735, 154.6605], [39.585903, 154.56618], [38.4313, 154.39905], [37.34572, 154.16039], [36.337418, 153.852], [35.41407, 153.47626], [34.582703, 153.036], [33.849644, 152.53459], [33.22047, 151.97583], [32.699974, 151.36397], [32.292118, 150.70369], [32.0, 150.0], [31.825848, 149.25826], [31.770985, 148.4841], [31.835829, 147.68343], [32.019886, 146.86234], [32.32176, 146.02707], [32.739143, 145.18399], [33.26887, 144.33951], [33.906902, 143.50006], [34.648388, 142.67201], [35.487682, 141.8617], [36.418396, 141.07526], [37.43345, 140.3187], [38.525112, 139.59775], [39.68508, 138.91792], [40.904526, 138.28438], [42.174168, 137.70195], [43.48434, 137.17506], [44.825077, 136.70773], [46.18617, 136.30351], [47.557262, 135.96547], [48.927917, 135.6962], [50.287704, 135.49773], [51.626278, 135.37157], [52.93345, 135.3187], [54.199265, 135.3395], [55.414097, 135.43382], [56.5687, 135.60095], [57.65428, 135.83961], [58.662582, 136.148], [59.58593, 136.52374], [60.417297, 136.964], [61.150356, 137.46541], [61.77953, 138.02417], [62.300026, 138.63603]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wQ2PjjWiMG4"
      },
      "source": [
        "im = np.zeros([512,512])\n",
        "for coordinates in list_coordinates:\n",
        "  \n",
        "  cv2.fillConvexPoly(im, np.array(coordinates, 'int32'), 255)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "px4ids64npr7",
        "outputId": "fde2fdaa-1e9c-469f-de56-d027157c6f6e"
      },
      "source": [
        "plt.imshow(im)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lecRaCbXQ7dH",
        "outputId": "15c5cee6-2be9-4eb0-c6fd-2740f707f16e"
      },
      "source": [
        "plt.hist(im.ravel(), 256, [0, 255])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([248521.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,      0.,      0.,      0.,      0.,\n",
              "             0.,      0.,      0.,  13623.]),\n",
              " array([  0.        ,   0.99609375,   1.9921875 ,   2.98828125,\n",
              "          3.984375  ,   4.98046875,   5.9765625 ,   6.97265625,\n",
              "          7.96875   ,   8.96484375,   9.9609375 ,  10.95703125,\n",
              "         11.953125  ,  12.94921875,  13.9453125 ,  14.94140625,\n",
              "         15.9375    ,  16.93359375,  17.9296875 ,  18.92578125,\n",
              "         19.921875  ,  20.91796875,  21.9140625 ,  22.91015625,\n",
              "         23.90625   ,  24.90234375,  25.8984375 ,  26.89453125,\n",
              "         27.890625  ,  28.88671875,  29.8828125 ,  30.87890625,\n",
              "         31.875     ,  32.87109375,  33.8671875 ,  34.86328125,\n",
              "         35.859375  ,  36.85546875,  37.8515625 ,  38.84765625,\n",
              "         39.84375   ,  40.83984375,  41.8359375 ,  42.83203125,\n",
              "         43.828125  ,  44.82421875,  45.8203125 ,  46.81640625,\n",
              "         47.8125    ,  48.80859375,  49.8046875 ,  50.80078125,\n",
              "         51.796875  ,  52.79296875,  53.7890625 ,  54.78515625,\n",
              "         55.78125   ,  56.77734375,  57.7734375 ,  58.76953125,\n",
              "         59.765625  ,  60.76171875,  61.7578125 ,  62.75390625,\n",
              "         63.75      ,  64.74609375,  65.7421875 ,  66.73828125,\n",
              "         67.734375  ,  68.73046875,  69.7265625 ,  70.72265625,\n",
              "         71.71875   ,  72.71484375,  73.7109375 ,  74.70703125,\n",
              "         75.703125  ,  76.69921875,  77.6953125 ,  78.69140625,\n",
              "         79.6875    ,  80.68359375,  81.6796875 ,  82.67578125,\n",
              "         83.671875  ,  84.66796875,  85.6640625 ,  86.66015625,\n",
              "         87.65625   ,  88.65234375,  89.6484375 ,  90.64453125,\n",
              "         91.640625  ,  92.63671875,  93.6328125 ,  94.62890625,\n",
              "         95.625     ,  96.62109375,  97.6171875 ,  98.61328125,\n",
              "         99.609375  , 100.60546875, 101.6015625 , 102.59765625,\n",
              "        103.59375   , 104.58984375, 105.5859375 , 106.58203125,\n",
              "        107.578125  , 108.57421875, 109.5703125 , 110.56640625,\n",
              "        111.5625    , 112.55859375, 113.5546875 , 114.55078125,\n",
              "        115.546875  , 116.54296875, 117.5390625 , 118.53515625,\n",
              "        119.53125   , 120.52734375, 121.5234375 , 122.51953125,\n",
              "        123.515625  , 124.51171875, 125.5078125 , 126.50390625,\n",
              "        127.5       , 128.49609375, 129.4921875 , 130.48828125,\n",
              "        131.484375  , 132.48046875, 133.4765625 , 134.47265625,\n",
              "        135.46875   , 136.46484375, 137.4609375 , 138.45703125,\n",
              "        139.453125  , 140.44921875, 141.4453125 , 142.44140625,\n",
              "        143.4375    , 144.43359375, 145.4296875 , 146.42578125,\n",
              "        147.421875  , 148.41796875, 149.4140625 , 150.41015625,\n",
              "        151.40625   , 152.40234375, 153.3984375 , 154.39453125,\n",
              "        155.390625  , 156.38671875, 157.3828125 , 158.37890625,\n",
              "        159.375     , 160.37109375, 161.3671875 , 162.36328125,\n",
              "        163.359375  , 164.35546875, 165.3515625 , 166.34765625,\n",
              "        167.34375   , 168.33984375, 169.3359375 , 170.33203125,\n",
              "        171.328125  , 172.32421875, 173.3203125 , 174.31640625,\n",
              "        175.3125    , 176.30859375, 177.3046875 , 178.30078125,\n",
              "        179.296875  , 180.29296875, 181.2890625 , 182.28515625,\n",
              "        183.28125   , 184.27734375, 185.2734375 , 186.26953125,\n",
              "        187.265625  , 188.26171875, 189.2578125 , 190.25390625,\n",
              "        191.25      , 192.24609375, 193.2421875 , 194.23828125,\n",
              "        195.234375  , 196.23046875, 197.2265625 , 198.22265625,\n",
              "        199.21875   , 200.21484375, 201.2109375 , 202.20703125,\n",
              "        203.203125  , 204.19921875, 205.1953125 , 206.19140625,\n",
              "        207.1875    , 208.18359375, 209.1796875 , 210.17578125,\n",
              "        211.171875  , 212.16796875, 213.1640625 , 214.16015625,\n",
              "        215.15625   , 216.15234375, 217.1484375 , 218.14453125,\n",
              "        219.140625  , 220.13671875, 221.1328125 , 222.12890625,\n",
              "        223.125     , 224.12109375, 225.1171875 , 226.11328125,\n",
              "        227.109375  , 228.10546875, 229.1015625 , 230.09765625,\n",
              "        231.09375   , 232.08984375, 233.0859375 , 234.08203125,\n",
              "        235.078125  , 236.07421875, 237.0703125 , 238.06640625,\n",
              "        239.0625    , 240.05859375, 241.0546875 , 242.05078125,\n",
              "        243.046875  , 244.04296875, 245.0390625 , 246.03515625,\n",
              "        247.03125   , 248.02734375, 249.0234375 , 250.01953125,\n",
              "        251.015625  , 252.01171875, 253.0078125 , 254.00390625,\n",
              "        255.        ]),\n",
              " <a list of 256 Patch objects>)"
            ]
          },
          "metadata": {},
          "execution_count": 107
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYkAAAD4CAYAAAAZ1BptAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAARWElEQVR4nO3db6xdVZ3G8e8zrRDjP4p0GtI2U9S+qSaD2EATjXEkgcKbYoIGXkhjiDUREk2cxKovICqJTKIkJEiCoaEYRySooYl1Oh0kMb4AuSgChUHuIIQ2hVaK4MSoA/7mxVmNh8tZ917u7b2nvf1+kp2zz2+vvfZa2c15cvbe9zRVhSRJo/zDuAcgSTp+GRKSpC5DQpLUZUhIkroMCUlS1/JxD+BYO+OMM2rdunXjHoYknVAefPDB31fVyqn1JRcS69atY2JiYtzDkKQTSpJnRtW93CRJ6jIkJEldhoQkqcuQkCR1zRgSSdYmuTfJY0n2Jflcq1+b5ECSh9py8dA+X0oymeSJJBcO1Te32mSS7UP1s5Lc3+o/SHJKq5/a3k+27euO5eQlSdObzTeJV4AvVNUGYBNwVZINbdsNVXV2W3YDtG2XAe8FNgPfTrIsyTLgJuAiYANw+VA/17e+3gO8CFzZ6lcCL7b6Da2dJGmRzBgSVXWwqn7V1v8IPA6snmaXLcAdVfWXqvodMAmc25bJqnqqqv4K3AFsSRLgo8Bdbf+dwCVDfe1s63cB57f2kqRF8IbuSbTLPe8H7m+lq5M8nGRHkhWtthp4dmi3/a3Wq78T+ENVvTKl/pq+2vaXWvup49qWZCLJxOHDh9/IlCRJ05h1SCR5K/BD4PNV9TJwM/Bu4GzgIPDNBRnhLFTVLVW1sao2rlz5uj8YlCTN0axCIsmbGATE96rqRwBV9XxVvVpVfwO+w+ByEsABYO3Q7mtarVd/ATgtyfIp9df01ba/o7VfEOu2/2ShupakE9Jsnm4KcCvweFV9a6h+5lCzjwGPtvVdwGXtyaSzgPXAL4EHgPXtSaZTGNzc3lWD/xrvXuDStv9W4O6hvra29UuBn5X/lZ4kLZrZ/HbTB4FPAo8keajVvszg6aSzgQKeBj4DUFX7ktwJPMbgyairqupVgCRXA3uAZcCOqtrX+vsicEeSrwO/ZhBKtNfvJpkEjjAIFknSIpkxJKrqF8CoJ4p2T7PPdcB1I+q7R+1XVU/x98tVw/U/Ax+faYySpIXhX1xLkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6ZgyJJGuT3JvksST7knyu1U9PsjfJk+11RasnyY1JJpM8nOScob62tvZPJtk6VP9AkkfaPjcmyXTHkCQtjtl8k3gF+EJVbQA2AVcl2QBsB+6pqvXAPe09wEXA+rZsA26GwQc+cA1wHnAucM3Qh/7NwKeH9tvc6r1jSJIWwYwhUVUHq+pXbf2PwOPAamALsLM12wlc0ta3ALfXwH3AaUnOBC4E9lbVkap6EdgLbG7b3l5V91VVAbdP6WvUMSRJi+AN3ZNIsg54P3A/sKqqDrZNzwGr2vpq4Nmh3fa32nT1/SPqTHOMqePalmQiycThw4ffyJQkSdOYdUgkeSvwQ+DzVfXy8Lb2DaCO8dheY7pjVNUtVbWxqjauXLlyIYchSSeVWYVEkjcxCIjvVdWPWvn5dqmI9nqo1Q8Aa4d2X9Nq09XXjKhPdwxJ0iKYzdNNAW4FHq+qbw1t2gUcfUJpK3D3UP2K9pTTJuCldsloD3BBkhXthvUFwJ627eUkm9qxrpjS16hjSJIWwfJZtPkg8EngkSQPtdqXgW8Adya5EngG+ETbthu4GJgE/gR8CqCqjiT5GvBAa/fVqjrS1j8L3Aa8GfhpW5jmGJKkRTBjSFTVL4B0Np8/on0BV3X62gHsGFGfAN43ov7CqGNIkhaHf3EtSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqmjEkkuxIcijJo0O1a5McSPJQWy4e2valJJNJnkhy4VB9c6tNJtk+VD8ryf2t/oMkp7T6qe39ZNu+7lhNWpI0O7P5JnEbsHlE/YaqOrstuwGSbAAuA97b9vl2kmVJlgE3ARcBG4DLW1uA61tf7wFeBK5s9SuBF1v9htZOkrSIZgyJqvo5cGSW/W0B7qiqv1TV74BJ4Ny2TFbVU1X1V+AOYEuSAB8F7mr77wQuGeprZ1u/Czi/tZckLZL53JO4OsnD7XLUilZbDTw71GZ/q/Xq7wT+UFWvTKm/pq+2/aXW/nWSbEsykWTi8OHD85iSJGnYXEPiZuDdwNnAQeCbx2xEc1BVt1TVxqrauHLlynEORZKWlDmFRFU9X1WvVtXfgO8wuJwEcABYO9R0Tav16i8ApyVZPqX+mr7a9ne09pKkRTKnkEhy5tDbjwFHn3zaBVzWnkw6C1gP/BJ4AFjfnmQ6hcHN7V1VVcC9wKVt/63A3UN9bW3rlwI/a+0lSYtk+UwNknwf+AhwRpL9wDXAR5KcDRTwNPAZgKral+RO4DHgFeCqqnq19XM1sAdYBuyoqn3tEF8E7kjydeDXwK2tfivw3SSTDG6cXzbv2UqS3pAZQ6KqLh9RvnVE7Wj764DrRtR3A7tH1J/i75erhut/Bj4+0/gkSQvHv7iWJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqWvGkEiyI8mhJI8O1U5PsjfJk+11RasnyY1JJpM8nOScoX22tvZPJtk6VP9AkkfaPjcmyXTHkCQtntl8k7gN2Dylth24p6rWA/e09wAXAevbsg24GQYf+MA1wHnAucA1Qx/6NwOfHtpv8wzHkCQtkhlDoqp+DhyZUt4C7GzrO4FLhuq318B9wGlJzgQuBPZW1ZGqehHYC2xu295eVfdVVQG3T+lr1DEkSYtkrvckVlXVwbb+HLCqra8Gnh1qt7/VpqvvH1Gf7hivk2RbkokkE4cPH57DdCRJo8z7xnX7BlDHYCxzPkZV3VJVG6tq48qVKxdyKJJ0UplrSDzfLhXRXg+1+gFg7VC7Na02XX3NiPp0x5AkLZK5hsQu4OgTSluBu4fqV7SnnDYBL7VLRnuAC5KsaDesLwD2tG0vJ9nUnmq6Ykpfo44hSVoky2dqkOT7wEeAM5LsZ/CU0jeAO5NcCTwDfKI13w1cDEwCfwI+BVBVR5J8DXigtftqVR29Gf5ZBk9QvRn4aVuY5hiSpEUyY0hU1eWdTeePaFvAVZ1+dgA7RtQngPeNqL8w6hiSpMXjX1xLkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK65hUSSZ5O8kiSh5JMtNrpSfYmebK9rmj1JLkxyWSSh5OcM9TP1tb+ySRbh+ofaP1Ptn0zn/FKkt6YY/FN4l+q6uyq2tjebwfuqar1wD3tPcBFwPq2bANuhkGoANcA5wHnAtccDZbW5tND+20+BuOVJM3SQlxu2gLsbOs7gUuG6rfXwH3AaUnOBC4E9lbVkap6EdgLbG7b3l5V91VVAbcP9SVJWgTzDYkC/jPJg0m2tdqqqjrY1p8DVrX11cCzQ/vub7Xp6vtH1F8nybYkE0kmDh8+PJ/5SJKGLJ/n/h+qqgNJ/hHYm+S/hzdWVSWpeR5jRlV1C3ALwMaNGxf8eJJ0spjXN4mqOtBeDwE/ZnBP4fl2qYj2eqg1PwCsHdp9TatNV18zoi5JWiRzDokkb0nytqPrwAXAo8Au4OgTSluBu9v6LuCK9pTTJuCldllqD3BBkhXthvUFwJ627eUkm9pTTVcM9SVJWgTzudy0Cvhxeyp1OfDvVfUfSR4A7kxyJfAM8InWfjdwMTAJ/An4FEBVHUnyNeCB1u6rVXWkrX8WuA14M/DTtkiSFsmcQ6KqngL+eUT9BeD8EfUCrur0tQPYMaI+AbxvrmOUJM2Pf3EtSeoyJCRJXYaEJKnLkJAkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV2GhCSpy5CQJHUZEpKkLkNCktRlSEiSugwJSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkroMCUlSlyEhSSe4ddt/wrrtP1mQvg0JSVKXISFJ6jIkJEldhoQkqcuQkCR1GRKSpC5DQpLUZUhIkrqO+5BIsjnJE0kmk2wf93gk6WRyXIdEkmXATcBFwAbg8iQbxjsqSTp5HNchAZwLTFbVU1X1V+AOYMuYxyRJJ43l4x7ADFYDzw693w+cN7VRkm3Atvb2f5M8McfjnZHr+f0c9z0RnQHOd4k6meYKzheAXD+vPv9pVPF4D4lZqapbgFvm20+SiaraeAyGdEJwvkvXyTRXcL4L6Xi/3HQAWDv0fk2rSZIWwfEeEg8A65OcleQU4DJg15jHJEknjeP6clNVvZLkamAPsAzYUVX7FvCQ875kdYJxvkvXyTRXcL4LJlW1WMeSJJ1gjvfLTZKkMTIkJEldhkSz1H/+I8nTSR5J8lCSiVY7PcneJE+21xXjHudcJdmR5FCSR4dqI+eXgRvbuX44yTnjG/ncdOZ7bZID7Rw/lOTioW1favN9IsmF4xn13CRZm+TeJI8l2Zfkc62+JM/vNPMdz/mtqpN+YXBT/H+AdwGnAL8BNox7XMd4jk8DZ0yp/Ruwva1vB64f9zjnMb8PA+cAj840P+Bi4KdAgE3A/eMe/zGa77XAv45ou6H9mz4VOKv9W1827jm8gbmeCZzT1t8G/LbNaUme32nmO5bz6zeJgZP15z+2ADvb+k7gkjGOZV6q6ufAkSnl3vy2ALfXwH3AaUnOXJyRHhud+fZsAe6oqr9U1e+ASQb/5k8IVXWwqn7V1v8IPM7g1xiW5PmdZr49C3p+DYmBUT//Md1JOREV8J9JHmw/YwKwqqoOtvXngFXjGdqC6c1vKZ/vq9sllh1Dlw+XzHyTrAPeD9zPSXB+p8wXxnB+DYmTx4eq6hwGv6h7VZIPD2+swffWJfs89FKfX3Mz8G7gbOAg8M3xDufYSvJW4IfA56vq5eFtS/H8jpjvWM6vITGw5H/+o6oOtNdDwI8ZfB19/ujX8PZ6aHwjXBC9+S3J811Vz1fVq1X1N+A7/P2Swwk/3yRvYvCB+b2q+lErL9nzO2q+4zq/hsTAkv75jyRvSfK2o+vABcCjDOa4tTXbCtw9nhEumN78dgFXtKdgNgEvDV22OGFNue7+MQbnGAbzvSzJqUnOAtYDv1zs8c1VkgC3Ao9X1beGNi3J89ub79jO77jv5B8vC4MnIn7L4MmAr4x7PMd4bu9i8PTDb4B9R+cHvBO4B3gS+C/g9HGPdR5z/D6Dr+D/x+Ca7JW9+TF46uWmdq4fATaOe/zHaL7fbfN5uH1wnDnU/ittvk8AF417/G9wrh9icCnpYeChtly8VM/vNPMdy/n1ZzkkSV1ebpIkdRkSkqQuQ0KS1GVISJK6DAlJUpchIUnqMiQkSV3/D9LnOlmSqezVAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NBlH2p8pB3R"
      },
      "source": [
        "src=\"/content/drive/MyDrive/Colab Notebooks/spermdata/spermdata/ROI/labels\"\n",
        "os.chdir(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTRucatmoHgb",
        "outputId": "09003f20-faa2-4a04-d042-9e954bcda871"
      },
      "source": [
        "cv2.imwrite('Image 39.png', im)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 345
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrgbvEiB4lzA"
      },
      "source": [
        "# Test the groundtruth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzt452bA4qVe"
      },
      "source": [
        "import os\n",
        "src=\"/content/drive/MyDrive/Colab Notebooks/spermdata/spermdata/ROI/labels\"\n",
        "os.chdir(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qmHNTTTH4t5d"
      },
      "source": [
        "im=cv2.imread('Image39.png')\n",
        "plt.hist(im.ravel(), 256, [0, 255])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SJqQYdp39Jj"
      },
      "source": [
        "#Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVsDOHLI8vWv"
      },
      "source": [
        "import os\n",
        "src=\"/content/drive/MyDrive/Colab Notebooks/spermdata/spermdata/\"\n",
        "os.chdir(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFgsLTcl4CkX"
      },
      "source": [
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "class SegDataset(Dataset):\n",
        "  def __init__(self, traintxt, imagesize, cropsize,transform=None):\n",
        "    self.images = []\n",
        "    self.labels = []\n",
        "\n",
        "    lines = open(traintxt,'r').readlines()\n",
        "    for line in lines:\n",
        "      imagepath,labelpath = line.strip().split(' ')\n",
        "      self.images.append(imagepath)\n",
        "      self.labels.append(labelpath)\n",
        "\n",
        "      self.imagesize = imagesize\n",
        "      self.cropsize = cropsize\n",
        "\n",
        "      assert len(self.images) == len(self.labels)\n",
        "      self.transform  = transform\n",
        "      self.samples = []\n",
        "      for i in range(len(self.images)):\n",
        "          self.samples.append((self.images[i],self.labels[i]))\n",
        "\n",
        "  def __getitem__(self, item):\n",
        "    img_path, label_path = self.samples[item]\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (self.imagesize,self.imagesize),interpolation=cv2.INTER_NEAREST)\n",
        "    label = cv2.imread(label_path,0)/255\n",
        "    label = cv2.resize(label, (self.imagesize, self.imagesize), interpolation=cv2.INTER_NEAREST)\n",
        "      ## \n",
        "    #randoffsetx = np.random.randint(self.imagesize - self.cropsize)\n",
        "    #randoffsety = np.random.randint(self.imagesize - self.cropsize)\n",
        "    #img = img[randoffsety:randoffsety + self.cropsize, randoffsetx:randoffsetx + self.cropsize]\n",
        "    #label = label[randoffsety:randoffsety + self.cropsize, randoffsetx:randoffsetx + self.cropsize]\n",
        "\n",
        "    if self.transform is not None:\n",
        "      img = self.transform(img)\n",
        "    return img, label\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRq1VfEE4crB"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWXl_GuT4gMp"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "class simpleNet5(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(simpleNet5, self).__init__()\n",
        "    self.conv1 = nn.Sequential(\n",
        "      nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1),\n",
        "      nn.BatchNorm2d(32),\n",
        "      nn.ReLU(True),)\n",
        "    self.conv2 = nn.Sequential(\n",
        "        nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "    self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "    self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "    self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "        )\n",
        "    self.deconv1 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(512, 256, 3, 2, 1, 1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "    self.deconv2 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 3, 2, 1, 1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "    self.deconv3 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, 3, 2, 1, 1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "    self.deconv4 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, 3, 2, 1, 1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "    self.deconv5 = nn.Sequential(\n",
        "            nn.ConvTranspose2d(32, 8, 3, 2, 1, 1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "    self.classifier = nn.Conv2d(8, 1, kernel_size=1)\n",
        "\n",
        "  def forward(self, x):       \n",
        "    out1 = self.conv1(x)     \n",
        "    out2 = self.conv2(out1)     \n",
        "    out3 = self.conv3(out2)   \n",
        "    out4 = self.conv4(out3)\n",
        "    out5 = self.conv5(out4)\n",
        "    out6 = self.deconv1(out5) +out4\n",
        "    out7 = self.deconv2(out6)+out3\n",
        "    out8 = self.deconv3(out7)+out2\n",
        "    out9 = self.deconv4(out8)+out1\n",
        "    out = self.deconv5(out9)\n",
        "    out = self.classifier(out)\n",
        "    out = torch.sigmoid(out)\n",
        "    return out\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UygT1XGNfth"
      },
      "source": [
        "class up_conv(nn.Module):\n",
        "    def __init__(self,ch_in,ch_out):\n",
        "        super(up_conv,self).__init__()\n",
        "        self.up = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
        "\t\t    nn.BatchNorm2d(ch_out),\n",
        "\t\t\tnn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.up(x)\n",
        "        return x\n",
        "class Recurrent_block(nn.Module):\n",
        "    def __init__(self, ch_out, t=2):\n",
        "        super(Recurrent_block, self).__init__()\n",
        "        self.t = t\n",
        "        self.ch_out = ch_out\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(ch_out, ch_out, kernel_size=3, stride=1, padding=1, bias=True),\n",
        "            nn.BatchNorm2d(ch_out),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(self.t):\n",
        "\n",
        "            if i == 0:\n",
        "                x1 = self.conv(x)\n",
        "\n",
        "            x1 = self.conv(x + x1)\n",
        "        return x1\n",
        "\n",
        "class RRCNN_block(nn.Module):\n",
        "    def __init__(self,ch_in,ch_out,t=2):\n",
        "        super(RRCNN_block,self).__init__()\n",
        "        self.RCNN = nn.Sequential(\n",
        "            Recurrent_block(ch_out,t=t),\n",
        "            Recurrent_block(ch_out,t=t)\n",
        "        )\n",
        "        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x = self.Conv_1x1(x)\n",
        "        x1 = self.RCNN(x)\n",
        "        return x+x1\n",
        "class R2U_Net(nn.Module):\n",
        "    def __init__(self, img_ch=3, output_ch=1, t=2):\n",
        "        super(R2U_Net, self).__init__()\n",
        "\n",
        "        self.Maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.Upsample = nn.Upsample(scale_factor=2)\n",
        "\n",
        "        self.RRCNN1 = RRCNN_block(ch_in=img_ch, ch_out=64, t=t)\n",
        "\n",
        "        self.RRCNN2 = RRCNN_block(ch_in=64, ch_out=128, t=t)\n",
        "\n",
        "        self.RRCNN3 = RRCNN_block(ch_in=128, ch_out=256, t=t)\n",
        "\n",
        "        self.RRCNN4 = RRCNN_block(ch_in=256, ch_out=512, t=t)\n",
        "\n",
        "        self.RRCNN5 = RRCNN_block(ch_in=512, ch_out=1024, t=t)\n",
        "\n",
        "        self.Up5 = up_conv(ch_in=1024, ch_out=512)\n",
        "        self.Up_RRCNN5 = RRCNN_block(ch_in=1024, ch_out=512, t=t)\n",
        "\n",
        "        self.Up4 = up_conv(ch_in=512, ch_out=256)\n",
        "        self.Up_RRCNN4 = RRCNN_block(ch_in=512, ch_out=256, t=t)\n",
        "\n",
        "        self.Up3 = up_conv(ch_in=256, ch_out=128)\n",
        "        self.Up_RRCNN3 = RRCNN_block(ch_in=256, ch_out=128, t=t)\n",
        "\n",
        "        self.Up2 = up_conv(ch_in=128, ch_out=64)\n",
        "        self.Up_RRCNN2 = RRCNN_block(ch_in=128, ch_out=64, t=t)\n",
        "\n",
        "        self.Conv_1x1 = nn.Conv2d(64, output_ch, kernel_size=1, stride=1, padding=0)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "    def forward(self, x):\n",
        "        # encoding path\n",
        "        x1 = self.RRCNN1(x)\n",
        "\n",
        "        x2 = self.Maxpool(x1)\n",
        "        x2 = self.RRCNN2(x2)\n",
        "\n",
        "        x3 = self.Maxpool(x2)\n",
        "        x3 = self.RRCNN3(x3)\n",
        "\n",
        "        x4 = self.Maxpool(x3)\n",
        "        x4 = self.RRCNN4(x4)\n",
        "\n",
        "        x5 = self.Maxpool(x4)\n",
        "        x5 = self.RRCNN5(x5)\n",
        "\n",
        "        # decoding + concat path\n",
        "        d5 = self.Up5(x5)\n",
        "        d5 = torch.cat((x4, d5), dim=1)\n",
        "        d5 = self.Up_RRCNN5(d5)\n",
        "\n",
        "        d4 = self.Up4(d5)\n",
        "        d4 = torch.cat((x3, d4), dim=1)\n",
        "        d4 = self.Up_RRCNN4(d4)\n",
        "\n",
        "        d3 = self.Up3(d4)\n",
        "        d3 = torch.cat((x2, d3), dim=1)\n",
        "        d3 = self.Up_RRCNN3(d3)\n",
        "\n",
        "        d2 = self.Up2(d3)\n",
        "        d2 = torch.cat((x1, d2), dim=1)\n",
        "        d2 = self.Up_RRCNN2(d2)\n",
        "\n",
        "        d1 = self.Conv_1x1(d2)\n",
        "        d1 = self.sigmoid(d1)\n",
        "        return d132w\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uxg001cz5Byf"
      },
      "source": [
        "#Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJ0kRIKu6dfl",
        "outputId": "21ab5ac3-20b7-468e-fd12-8283f3df022e"
      },
      "source": [
        "!pip install tensorboardX"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[?25l\r\u001b[K     |                             | 10 kB 35.3 MB/s eta 0:00:01\r\u001b[K     |                          | 20 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |                        | 30 kB 17.4 MB/s eta 0:00:01\r\u001b[K     |                     | 40 kB 14.7 MB/s eta 0:00:01\r\u001b[K     |                  | 51 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |                | 61 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |             | 71 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |           | 81 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |        | 92 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |     | 102 kB 7.4 MB/s eta 0:00:01\r\u001b[K     |   | 112 kB 7.4 MB/s eta 0:00:01\r\u001b[K     || 122 kB 7.4 MB/s eta 0:00:01\r\u001b[K     || 124 kB 7.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fklTg_5vOC6d"
      },
      "source": [
        "src=\"/content/drive/MyDrive/Colab Notebooks/spermdata/spermdata/\"\n",
        "os.chdir(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DoOUGUIv5Do_",
        "outputId": "54666d38-4659-47f3-cb9e-42c57f6619fc"
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "from tensorboardX import SummaryWriter\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "\n",
        "writer = SummaryWriter() #\n",
        "\n",
        "batchsize = 8\n",
        "epochs = 200\n",
        "imagesize = 512 #\n",
        "cropsize = 64 #\n",
        "train_data_path = './train.txt'\n",
        "val_data_path = './valid.txt'\n",
        "\n",
        "# \n",
        "data_transform = transforms.Compose([\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])\n",
        "\n",
        "\n",
        "# \n",
        "train_dataset = SegDataset(train_data_path,imagesize,cropsize,data_transform)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batchsize, shuffle=True)\n",
        "val_dataset = SegDataset(val_data_path,imagesize,cropsize,data_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=val_dataset.__len__(), shuffle=True)\n",
        "\n",
        "image_datasets = {}\n",
        "image_datasets['train'] = train_dataset\n",
        "image_datasets['val'] = val_dataset\n",
        "dataloaders = {}\n",
        "dataloaders['train'] = train_dataloader\n",
        "dataloaders['val'] = val_dataloader\n",
        "\n",
        "device = torch.device('cpu')\n",
        "net = simpleNet5().to(device)\n",
        "#net = U_Net().to(device)\n",
        "\n",
        "criterion = nn.BCELoss() #softmax losslabel\n",
        "optimizer = optim.SGD(net.parameters(), lr=1e-1, momentum=0.9)\n",
        "scheduler = lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1) #50epoch\n",
        "\n",
        "if not os.path.exists('checkpoints'):\n",
        "  os.mkdir('checkpoints')\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  print('Epoch {}/{}'.format(epoch, epochs - 1))\n",
        "  for phase in ['train', 'val']:\n",
        "    if phase == 'train':\n",
        "      scheduler.step()\n",
        "      net.train(True)  # Set model to training mode\n",
        "    else:\n",
        "      net.train(False)  # Set model to evaluate mode\n",
        "\n",
        "    running_loss = 0.0\n",
        "    running_accs = 0.0\n",
        "\n",
        "    n = 0\n",
        "    for data in dataloaders[phase]:\n",
        "      imgs, labels = data\n",
        "      img, label = imgs.to(device).float(), labels.to(device).float()\n",
        "      output = net(img)\n",
        "      output= torch.squeeze(output)\n",
        "      loss = criterion(output, label) \n",
        "      output_mask = output.data.numpy().copy()\n",
        "      y_mask = label.data.numpy().copy()\n",
        "      output_mask=(output_mask>0.5)\n",
        "      output=(output>0.5)\n",
        "      inter = torch.dot(output.view(-1).float(),label.view(-1))\n",
        "      union = torch.sum(output.view(-1).float()) + torch.sum(label.view(-1)) - inter + 0.0001\n",
        "        # Calculate DICE \n",
        "      acc= inter / union\n",
        "      \n",
        "      #acc = (output_mask == y_mask)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      if phase == 'train':\n",
        "            # 0\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "      running_loss += loss.data.item()\n",
        "      running_accs += acc\n",
        "      n += 1\n",
        "\n",
        "    epoch_loss = running_loss / n\n",
        "    epoch_acc = running_accs / n\n",
        "\n",
        "    if phase == 'train':\n",
        "      writer.add_scalar('data/trainloss', epoch_loss, epoch)\n",
        "      writer.add_scalar('data/trainacc', epoch_acc, epoch)\n",
        "      print('train epoch_{} loss='+str(epoch_loss).format(epoch))\n",
        "      print('train epoch_{} dice='+str(epoch_acc).format(epoch))\n",
        "    else:\n",
        "      writer.add_scalar('data/valloss', epoch_loss, epoch)\n",
        "      writer.add_scalar('data/valacc', epoch_acc, epoch)\n",
        "      print('val epoch_{} loss='+str(epoch_loss).format(epoch))\n",
        "      print('val epoch_{} dice='+str(epoch_acc).format(epoch))\n",
        "\n",
        "\n",
        "if epoch % 10 == 0:\n",
        "\n",
        "  torch.save(net, 'checkpoints/model_epoch_{}.pth'.format(epoch))\n",
        "  print('checkpoints/model_epoch_{}.pth saved!'.format(epoch))\n",
        "\n",
        "writer.export_scalars_to_json(\"./all_scalars.json\")\n",
        "writer.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/199\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train epoch_{} loss=0.49076901078224183\n",
            "train epoch_{} dice=tensor(0.0008)\n",
            "val epoch_{} loss=0.47023892402648926\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 2/199\n",
            "train epoch_{} loss=0.2984369397163391\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.26929762959480286\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 3/199\n",
            "train epoch_{} loss=0.2433178186416626\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.19708840548992157\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 4/199\n",
            "train epoch_{} loss=0.25559017062187195\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.18808358907699585\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 5/199\n",
            "train epoch_{} loss=0.25409910678863523\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.18915559351444244\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 6/199\n",
            "train epoch_{} loss=0.23810153007507323\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.18828651309013367\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 7/199\n",
            "train epoch_{} loss=0.2264874279499054\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.18552246689796448\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 8/199\n",
            "train epoch_{} loss=0.21959724724292756\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.18240228295326233\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 9/199\n",
            "train epoch_{} loss=0.22013671994209288\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.17970463633537292\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 10/199\n",
            "train epoch_{} loss=0.21839625239372254\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1752261370420456\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 11/199\n",
            "train epoch_{} loss=0.2114982008934021\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.17134889960289001\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 12/199\n",
            "train epoch_{} loss=0.20515807271003722\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.16782206296920776\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 13/199\n",
            "train epoch_{} loss=0.20594734847545623\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1681661456823349\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 14/199\n",
            "train epoch_{} loss=0.19899024367332457\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1632733941078186\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 15/199\n",
            "train epoch_{} loss=0.20012754797935486\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.16189783811569214\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 16/199\n",
            "train epoch_{} loss=0.19809083342552186\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.16594596207141876\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 17/199\n",
            "train epoch_{} loss=0.19431764483451844\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.15846270322799683\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 18/199\n",
            "train epoch_{} loss=0.19391857087612152\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.17322765290737152\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 19/199\n",
            "train epoch_{} loss=0.192173370718956\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.16029340028762817\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 20/199\n",
            "train epoch_{} loss=0.1885770559310913\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.16059130430221558\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 21/199\n",
            "train epoch_{} loss=0.182994344830513\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1494462937116623\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 22/199\n",
            "train epoch_{} loss=0.18014506697654725\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1465255469083786\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 23/199\n",
            "train epoch_{} loss=0.1812642455101013\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.15340788662433624\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 24/199\n",
            "train epoch_{} loss=0.17265718877315522\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1524716168642044\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 25/199\n",
            "train epoch_{} loss=0.16767192780971527\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.13974826037883759\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 26/199\n",
            "train epoch_{} loss=0.1637786716222763\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.14275267720222473\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 27/199\n",
            "train epoch_{} loss=0.15756974518299102\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.17302033305168152\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 28/199\n",
            "train epoch_{} loss=0.15378545224666595\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.12970882654190063\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 29/199\n",
            "train epoch_{} loss=0.1471063494682312\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.12897592782974243\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 30/199\n",
            "train epoch_{} loss=0.13898379653692244\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.12973704934120178\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 31/199\n",
            "train epoch_{} loss=0.13231794089078902\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1277686208486557\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 32/199\n",
            "train epoch_{} loss=0.12406774461269379\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.12378830462694168\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 33/199\n",
            "train epoch_{} loss=0.11660913228988648\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.12974798679351807\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 34/199\n",
            "train epoch_{} loss=0.11104163527488708\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.12757770717144012\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 35/199\n",
            "train epoch_{} loss=0.10596713572740554\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.12856148183345795\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 36/199\n",
            "train epoch_{} loss=0.09918088912963867\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1411738395690918\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 37/199\n",
            "train epoch_{} loss=0.09267606437206269\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1489134579896927\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 38/199\n",
            "train epoch_{} loss=0.08537844270467758\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1555512249469757\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 39/199\n",
            "train epoch_{} loss=0.07886441498994827\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.1524408757686615\n",
            "val epoch_{} dice=tensor(0.)\n",
            "Epoch 40/199\n",
            "train epoch_{} loss=0.07394747734069824\n",
            "train epoch_{} dice=tensor(0.)\n",
            "val epoch_{} loss=0.16255486011505127\n",
            "val epoch_{} dice=tensor(0.0970)\n",
            "Epoch 41/199\n",
            "train epoch_{} loss=0.06907085478305816\n",
            "train epoch_{} dice=tensor(0.7156)\n",
            "val epoch_{} loss=0.16301970183849335\n",
            "val epoch_{} dice=tensor(0.1420)\n",
            "Epoch 42/199\n",
            "train epoch_{} loss=0.06834125965833664\n",
            "train epoch_{} dice=tensor(0.7429)\n",
            "val epoch_{} loss=0.15840834379196167\n",
            "val epoch_{} dice=tensor(0.1457)\n",
            "Epoch 43/199\n",
            "train epoch_{} loss=0.06227434724569321\n",
            "train epoch_{} dice=tensor(0.7791)\n",
            "val epoch_{} loss=0.14736750721931458\n",
            "val epoch_{} dice=tensor(0.2006)\n",
            "Epoch 44/199\n",
            "train epoch_{} loss=0.0590499296784401\n",
            "train epoch_{} dice=tensor(0.7882)\n",
            "val epoch_{} loss=0.14355355501174927\n",
            "val epoch_{} dice=tensor(0.2244)\n",
            "Epoch 45/199\n",
            "train epoch_{} loss=0.06648144125938416\n",
            "train epoch_{} dice=tensor(0.7671)\n",
            "val epoch_{} loss=0.15384624898433685\n",
            "val epoch_{} dice=tensor(0.2224)\n",
            "Epoch 46/199\n",
            "train epoch_{} loss=0.055487587302923205\n",
            "train epoch_{} dice=tensor(0.7944)\n",
            "val epoch_{} loss=0.13928267359733582\n",
            "val epoch_{} dice=tensor(0.2816)\n",
            "Epoch 47/199\n",
            "train epoch_{} loss=0.058993390202522276\n",
            "train epoch_{} dice=tensor(0.7648)\n",
            "val epoch_{} loss=0.1479722261428833\n",
            "val epoch_{} dice=tensor(0.2343)\n",
            "Epoch 48/199\n",
            "train epoch_{} loss=0.050653234124183655\n",
            "train epoch_{} dice=tensor(0.8181)\n",
            "val epoch_{} loss=0.1462651491165161\n",
            "val epoch_{} dice=tensor(0.2817)\n",
            "Epoch 49/199\n",
            "train epoch_{} loss=0.046534255146980286\n",
            "train epoch_{} dice=tensor(0.8272)\n",
            "val epoch_{} loss=0.1432817280292511\n",
            "val epoch_{} dice=tensor(0.2632)\n",
            "Epoch 50/199\n",
            "train epoch_{} loss=0.04597220569849014\n",
            "train epoch_{} dice=tensor(0.8314)\n",
            "val epoch_{} loss=0.14221099019050598\n",
            "val epoch_{} dice=tensor(0.2640)\n",
            "Epoch 51/199\n",
            "train epoch_{} loss=0.04402838945388794\n",
            "train epoch_{} dice=tensor(0.8471)\n",
            "val epoch_{} loss=0.14456892013549805\n",
            "val epoch_{} dice=tensor(0.2578)\n",
            "Epoch 52/199\n",
            "train epoch_{} loss=0.04517802447080612\n",
            "train epoch_{} dice=tensor(0.8396)\n",
            "val epoch_{} loss=0.14714190363883972\n",
            "val epoch_{} dice=tensor(0.2477)\n",
            "Epoch 53/199\n",
            "train epoch_{} loss=0.04311903268098831\n",
            "train epoch_{} dice=tensor(0.8555)\n",
            "val epoch_{} loss=0.14929364621639252\n",
            "val epoch_{} dice=tensor(0.2411)\n",
            "Epoch 54/199\n",
            "train epoch_{} loss=0.041972026228904724\n",
            "train epoch_{} dice=tensor(0.8575)\n",
            "val epoch_{} loss=0.1435854136943817\n",
            "val epoch_{} dice=tensor(0.2559)\n",
            "Epoch 55/199\n",
            "train epoch_{} loss=0.04104989096522331\n",
            "train epoch_{} dice=tensor(0.8694)\n",
            "val epoch_{} loss=0.14245058596134186\n",
            "val epoch_{} dice=tensor(0.2580)\n",
            "Epoch 56/199\n",
            "train epoch_{} loss=0.0411274679005146\n",
            "train epoch_{} dice=tensor(0.8710)\n",
            "val epoch_{} loss=0.1454342007637024\n",
            "val epoch_{} dice=tensor(0.2487)\n",
            "Epoch 57/199\n",
            "train epoch_{} loss=0.04203566312789917\n",
            "train epoch_{} dice=tensor(0.8584)\n",
            "val epoch_{} loss=0.1457604020833969\n",
            "val epoch_{} dice=tensor(0.2529)\n",
            "Epoch 58/199\n",
            "train epoch_{} loss=0.041068395227193834\n",
            "train epoch_{} dice=tensor(0.8653)\n",
            "val epoch_{} loss=0.1444075107574463\n",
            "val epoch_{} dice=tensor(0.2559)\n",
            "Epoch 59/199\n",
            "train epoch_{} loss=0.04106717109680176\n",
            "train epoch_{} dice=tensor(0.8637)\n",
            "val epoch_{} loss=0.14639943838119507\n",
            "val epoch_{} dice=tensor(0.2496)\n",
            "Epoch 60/199\n",
            "train epoch_{} loss=0.03966513276100159\n",
            "train epoch_{} dice=tensor(0.8720)\n",
            "val epoch_{} loss=0.14735326170921326\n",
            "val epoch_{} dice=tensor(0.2472)\n",
            "Epoch 61/199\n",
            "train epoch_{} loss=0.03894284442067146\n",
            "train epoch_{} dice=tensor(0.8757)\n",
            "val epoch_{} loss=0.14732006192207336\n",
            "val epoch_{} dice=tensor(0.2516)\n",
            "Epoch 62/199\n",
            "train epoch_{} loss=0.03919748067855835\n",
            "train epoch_{} dice=tensor(0.8775)\n",
            "val epoch_{} loss=0.1466999500989914\n",
            "val epoch_{} dice=tensor(0.2534)\n",
            "Epoch 63/199\n",
            "train epoch_{} loss=0.039307884871959686\n",
            "train epoch_{} dice=tensor(0.8764)\n",
            "val epoch_{} loss=0.1462092250585556\n",
            "val epoch_{} dice=tensor(0.2532)\n",
            "Epoch 64/199\n",
            "train epoch_{} loss=0.04037337228655815\n",
            "train epoch_{} dice=tensor(0.8711)\n",
            "val epoch_{} loss=0.14695855975151062\n",
            "val epoch_{} dice=tensor(0.2511)\n",
            "Epoch 65/199\n",
            "train epoch_{} loss=0.04009175598621369\n",
            "train epoch_{} dice=tensor(0.8637)\n",
            "val epoch_{} loss=0.14232710003852844\n",
            "val epoch_{} dice=tensor(0.2680)\n",
            "Epoch 66/199\n",
            "train epoch_{} loss=0.04206263348460197\n",
            "train epoch_{} dice=tensor(0.8606)\n",
            "val epoch_{} loss=0.14433014392852783\n",
            "val epoch_{} dice=tensor(0.2648)\n",
            "Epoch 67/199\n",
            "train epoch_{} loss=0.038995600491762164\n",
            "train epoch_{} dice=tensor(0.8760)\n",
            "val epoch_{} loss=0.14602409303188324\n",
            "val epoch_{} dice=tensor(0.2564)\n",
            "Epoch 68/199\n",
            "train epoch_{} loss=0.03981245458126068\n",
            "train epoch_{} dice=tensor(0.8616)\n",
            "val epoch_{} loss=0.14514638483524323\n",
            "val epoch_{} dice=tensor(0.2586)\n",
            "Epoch 69/199\n",
            "train epoch_{} loss=0.03815568089485168\n",
            "train epoch_{} dice=tensor(0.8799)\n",
            "val epoch_{} loss=0.1453126072883606\n",
            "val epoch_{} dice=tensor(0.2606)\n",
            "Epoch 70/199\n",
            "train epoch_{} loss=0.03950819447636604\n",
            "train epoch_{} dice=tensor(0.8656)\n",
            "val epoch_{} loss=0.14751625061035156\n",
            "val epoch_{} dice=tensor(0.2574)\n",
            "Epoch 71/199\n",
            "train epoch_{} loss=0.03769329786300659\n",
            "train epoch_{} dice=tensor(0.8829)\n",
            "val epoch_{} loss=0.15050897002220154\n",
            "val epoch_{} dice=tensor(0.2473)\n",
            "Epoch 72/199\n",
            "train epoch_{} loss=0.03678579106926918\n",
            "train epoch_{} dice=tensor(0.8862)\n",
            "val epoch_{} loss=0.1479642391204834\n",
            "val epoch_{} dice=tensor(0.2546)\n",
            "Epoch 73/199\n",
            "train epoch_{} loss=0.04140374138951301\n",
            "train epoch_{} dice=tensor(0.8508)\n",
            "val epoch_{} loss=0.1458265483379364\n",
            "val epoch_{} dice=tensor(0.2625)\n",
            "Epoch 74/199\n",
            "train epoch_{} loss=0.036747883260250094\n",
            "train epoch_{} dice=tensor(0.8878)\n",
            "val epoch_{} loss=0.14643189311027527\n",
            "val epoch_{} dice=tensor(0.2615)\n",
            "Epoch 75/199\n",
            "train epoch_{} loss=0.036164470762014386\n",
            "train epoch_{} dice=tensor(0.8866)\n",
            "val epoch_{} loss=0.14832469820976257\n",
            "val epoch_{} dice=tensor(0.2591)\n",
            "Epoch 76/199\n",
            "train epoch_{} loss=0.03662498891353607\n",
            "train epoch_{} dice=tensor(0.8808)\n",
            "val epoch_{} loss=0.14762184023857117\n",
            "val epoch_{} dice=tensor(0.2643)\n",
            "Epoch 77/199\n",
            "train epoch_{} loss=0.036778281629085544\n",
            "train epoch_{} dice=tensor(0.8843)\n",
            "val epoch_{} loss=0.14600276947021484\n",
            "val epoch_{} dice=tensor(0.2704)\n",
            "Epoch 78/199\n",
            "train epoch_{} loss=0.03727008029818535\n",
            "train epoch_{} dice=tensor(0.8841)\n",
            "val epoch_{} loss=0.14710330963134766\n",
            "val epoch_{} dice=tensor(0.2653)\n",
            "Epoch 79/199\n",
            "train epoch_{} loss=0.03624937534332275\n",
            "train epoch_{} dice=tensor(0.8921)\n",
            "val epoch_{} loss=0.14853209257125854\n",
            "val epoch_{} dice=tensor(0.2587)\n",
            "Epoch 80/199\n",
            "train epoch_{} loss=0.035251214355230334\n",
            "train epoch_{} dice=tensor(0.8913)\n",
            "val epoch_{} loss=0.1492043137550354\n",
            "val epoch_{} dice=tensor(0.2571)\n",
            "Epoch 81/199\n",
            "train epoch_{} loss=0.03585992604494095\n",
            "train epoch_{} dice=tensor(0.8907)\n",
            "val epoch_{} loss=0.1492883861064911\n",
            "val epoch_{} dice=tensor(0.2571)\n",
            "Epoch 82/199\n",
            "train epoch_{} loss=0.036344397068023684\n",
            "train epoch_{} dice=tensor(0.8754)\n",
            "val epoch_{} loss=0.1466742753982544\n",
            "val epoch_{} dice=tensor(0.2657)\n",
            "Epoch 83/199\n",
            "train epoch_{} loss=0.03456669226288796\n",
            "train epoch_{} dice=tensor(0.8947)\n",
            "val epoch_{} loss=0.14596295356750488\n",
            "val epoch_{} dice=tensor(0.2683)\n",
            "Epoch 84/199\n",
            "train epoch_{} loss=0.03499182648956776\n",
            "train epoch_{} dice=tensor(0.8892)\n",
            "val epoch_{} loss=0.1469832956790924\n",
            "val epoch_{} dice=tensor(0.2682)\n",
            "Epoch 85/199\n",
            "train epoch_{} loss=0.035589759796857835\n",
            "train epoch_{} dice=tensor(0.8881)\n",
            "val epoch_{} loss=0.1490410715341568\n",
            "val epoch_{} dice=tensor(0.2616)\n",
            "Epoch 86/199\n",
            "train epoch_{} loss=0.035427088290452956\n",
            "train epoch_{} dice=tensor(0.8863)\n",
            "val epoch_{} loss=0.14885282516479492\n",
            "val epoch_{} dice=tensor(0.2654)\n",
            "Epoch 87/199\n",
            "train epoch_{} loss=0.03471951335668564\n",
            "train epoch_{} dice=tensor(0.8939)\n",
            "val epoch_{} loss=0.14774638414382935\n",
            "val epoch_{} dice=tensor(0.2652)\n",
            "Epoch 88/199\n",
            "train epoch_{} loss=0.034851279109716415\n",
            "train epoch_{} dice=tensor(0.8886)\n",
            "val epoch_{} loss=0.1474210023880005\n",
            "val epoch_{} dice=tensor(0.2684)\n",
            "Epoch 89/199\n",
            "train epoch_{} loss=0.03437398001551628\n",
            "train epoch_{} dice=tensor(0.8916)\n",
            "val epoch_{} loss=0.14855502545833588\n",
            "val epoch_{} dice=tensor(0.2648)\n",
            "Epoch 90/199\n",
            "train epoch_{} loss=0.0352162092924118\n",
            "train epoch_{} dice=tensor(0.8894)\n",
            "val epoch_{} loss=0.15081802010536194\n",
            "val epoch_{} dice=tensor(0.2568)\n",
            "Epoch 91/199\n",
            "train epoch_{} loss=0.0352334126830101\n",
            "train epoch_{} dice=tensor(0.8848)\n",
            "val epoch_{} loss=0.1469648778438568\n",
            "val epoch_{} dice=tensor(0.2728)\n",
            "Epoch 92/199\n",
            "train epoch_{} loss=0.03375804349780083\n",
            "train epoch_{} dice=tensor(0.8968)\n",
            "val epoch_{} loss=0.14918065071105957\n",
            "val epoch_{} dice=tensor(0.2675)\n",
            "Epoch 93/199\n",
            "train epoch_{} loss=0.03359697535634041\n",
            "train epoch_{} dice=tensor(0.8990)\n",
            "val epoch_{} loss=0.15284058451652527\n",
            "val epoch_{} dice=tensor(0.2571)\n",
            "Epoch 94/199\n",
            "train epoch_{} loss=0.03331113010644913\n",
            "train epoch_{} dice=tensor(0.8940)\n",
            "val epoch_{} loss=0.15119148790836334\n",
            "val epoch_{} dice=tensor(0.2621)\n",
            "Epoch 95/199\n",
            "train epoch_{} loss=0.03396341428160667\n",
            "train epoch_{} dice=tensor(0.8939)\n",
            "val epoch_{} loss=0.1495661437511444\n",
            "val epoch_{} dice=tensor(0.2685)\n",
            "Epoch 96/199\n",
            "train epoch_{} loss=0.03313558995723724\n",
            "train epoch_{} dice=tensor(0.8934)\n",
            "val epoch_{} loss=0.14869657158851624\n",
            "val epoch_{} dice=tensor(0.2731)\n",
            "Epoch 97/199\n",
            "train epoch_{} loss=0.03382921256124973\n",
            "train epoch_{} dice=tensor(0.8957)\n",
            "val epoch_{} loss=0.1499711573123932\n",
            "val epoch_{} dice=tensor(0.2685)\n",
            "Epoch 98/199\n",
            "train epoch_{} loss=0.03436346426606178\n",
            "train epoch_{} dice=tensor(0.8896)\n",
            "val epoch_{} loss=0.1485530287027359\n",
            "val epoch_{} dice=tensor(0.2743)\n",
            "Epoch 99/199\n",
            "train epoch_{} loss=0.03295471966266632\n",
            "train epoch_{} dice=tensor(0.8990)\n",
            "val epoch_{} loss=0.14947956800460815\n",
            "val epoch_{} dice=tensor(0.2716)\n",
            "Epoch 100/199\n",
            "train epoch_{} loss=0.03469379916787148\n",
            "train epoch_{} dice=tensor(0.8831)\n",
            "val epoch_{} loss=0.149044930934906\n",
            "val epoch_{} dice=tensor(0.2699)\n",
            "Epoch 101/199\n",
            "train epoch_{} loss=0.03353983834385872\n",
            "train epoch_{} dice=tensor(0.8966)\n",
            "val epoch_{} loss=0.14979878067970276\n",
            "val epoch_{} dice=tensor(0.2695)\n",
            "Epoch 102/199\n",
            "train epoch_{} loss=0.03218357600271702\n",
            "train epoch_{} dice=tensor(0.9042)\n",
            "val epoch_{} loss=0.15021151304244995\n",
            "val epoch_{} dice=tensor(0.2656)\n",
            "Epoch 103/199\n",
            "train epoch_{} loss=0.03272363618016243\n",
            "train epoch_{} dice=tensor(0.9015)\n",
            "val epoch_{} loss=0.15054607391357422\n",
            "val epoch_{} dice=tensor(0.2634)\n",
            "Epoch 104/199\n",
            "train epoch_{} loss=0.035322881489992144\n",
            "train epoch_{} dice=tensor(0.8720)\n",
            "val epoch_{} loss=0.15002921223640442\n",
            "val epoch_{} dice=tensor(0.2657)\n",
            "Epoch 105/199\n",
            "train epoch_{} loss=0.032606279477477074\n",
            "train epoch_{} dice=tensor(0.8977)\n",
            "val epoch_{} loss=0.15040118992328644\n",
            "val epoch_{} dice=tensor(0.2652)\n",
            "Epoch 106/199\n",
            "train epoch_{} loss=0.03359014019370079\n",
            "train epoch_{} dice=tensor(0.8934)\n",
            "val epoch_{} loss=0.15040403604507446\n",
            "val epoch_{} dice=tensor(0.2652)\n",
            "Epoch 107/199\n",
            "train epoch_{} loss=0.032921872287988665\n",
            "train epoch_{} dice=tensor(0.8961)\n",
            "val epoch_{} loss=0.15018442273139954\n",
            "val epoch_{} dice=tensor(0.2673)\n",
            "Epoch 108/199\n",
            "train epoch_{} loss=0.03195904642343521\n",
            "train epoch_{} dice=tensor(0.9022)\n",
            "val epoch_{} loss=0.15032744407653809\n",
            "val epoch_{} dice=tensor(0.2662)\n",
            "Epoch 109/199\n",
            "train epoch_{} loss=0.03370421379804611\n",
            "train epoch_{} dice=tensor(0.8940)\n",
            "val epoch_{} loss=0.15106424689292908\n",
            "val epoch_{} dice=tensor(0.2657)\n",
            "Epoch 110/199\n",
            "train epoch_{} loss=0.03271597884595394\n",
            "train epoch_{} dice=tensor(0.8996)\n",
            "val epoch_{} loss=0.15122920274734497\n",
            "val epoch_{} dice=tensor(0.2649)\n",
            "Epoch 111/199\n",
            "train epoch_{} loss=0.03266858085989952\n",
            "train epoch_{} dice=tensor(0.8973)\n",
            "val epoch_{} loss=0.15047422051429749\n",
            "val epoch_{} dice=tensor(0.2664)\n",
            "Epoch 112/199\n",
            "train epoch_{} loss=0.03377072289586067\n",
            "train epoch_{} dice=tensor(0.8868)\n",
            "val epoch_{} loss=0.14971429109573364\n",
            "val epoch_{} dice=tensor(0.2694)\n",
            "Epoch 113/199\n",
            "train epoch_{} loss=0.031880146265029906\n",
            "train epoch_{} dice=tensor(0.9048)\n",
            "val epoch_{} loss=0.14971062541007996\n",
            "val epoch_{} dice=tensor(0.2692)\n",
            "Epoch 114/199\n",
            "train epoch_{} loss=0.033117524906992914\n",
            "train epoch_{} dice=tensor(0.8978)\n",
            "val epoch_{} loss=0.14986631274223328\n",
            "val epoch_{} dice=tensor(0.2684)\n",
            "Epoch 115/199\n",
            "train epoch_{} loss=0.03253624103963375\n",
            "train epoch_{} dice=tensor(0.9002)\n",
            "val epoch_{} loss=0.14947646856307983\n",
            "val epoch_{} dice=tensor(0.2687)\n",
            "Epoch 116/199\n",
            "train epoch_{} loss=0.032600730285048486\n",
            "train epoch_{} dice=tensor(0.9023)\n",
            "val epoch_{} loss=0.15058261156082153\n",
            "val epoch_{} dice=tensor(0.2650)\n",
            "Epoch 117/199\n",
            "train epoch_{} loss=0.03227027393877506\n",
            "train epoch_{} dice=tensor(0.8997)\n",
            "val epoch_{} loss=0.1505240499973297\n",
            "val epoch_{} dice=tensor(0.2673)\n",
            "Epoch 118/199\n",
            "train epoch_{} loss=0.03190895617008209\n",
            "train epoch_{} dice=tensor(0.9038)\n",
            "val epoch_{} loss=0.1508249044418335\n",
            "val epoch_{} dice=tensor(0.2665)\n",
            "Epoch 119/199\n",
            "train epoch_{} loss=0.03272336609661579\n",
            "train epoch_{} dice=tensor(0.9032)\n",
            "val epoch_{} loss=0.15164270997047424\n",
            "val epoch_{} dice=tensor(0.2632)\n",
            "Epoch 120/199\n",
            "train epoch_{} loss=0.033299470692873\n",
            "train epoch_{} dice=tensor(0.8964)\n",
            "val epoch_{} loss=0.15149356424808502\n",
            "val epoch_{} dice=tensor(0.2650)\n",
            "Epoch 121/199\n",
            "train epoch_{} loss=0.03338911011815071\n",
            "train epoch_{} dice=tensor(0.8953)\n",
            "val epoch_{} loss=0.15174439549446106\n",
            "val epoch_{} dice=tensor(0.2627)\n",
            "Epoch 122/199\n",
            "train epoch_{} loss=0.03216317817568779\n",
            "train epoch_{} dice=tensor(0.9009)\n",
            "val epoch_{} loss=0.15091446042060852\n",
            "val epoch_{} dice=tensor(0.2659)\n",
            "Epoch 123/199\n",
            "train epoch_{} loss=0.03231379613280296\n",
            "train epoch_{} dice=tensor(0.9028)\n",
            "val epoch_{} loss=0.15090414881706238\n",
            "val epoch_{} dice=tensor(0.2670)\n",
            "Epoch 124/199\n",
            "train epoch_{} loss=0.03325928561389446\n",
            "train epoch_{} dice=tensor(0.8908)\n",
            "val epoch_{} loss=0.1502629965543747\n",
            "val epoch_{} dice=tensor(0.2685)\n",
            "Epoch 125/199\n",
            "train epoch_{} loss=0.033228008449077605\n",
            "train epoch_{} dice=tensor(0.8990)\n",
            "val epoch_{} loss=0.15165376663208008\n",
            "val epoch_{} dice=tensor(0.2638)\n",
            "Epoch 126/199\n",
            "train epoch_{} loss=0.03310430943965912\n",
            "train epoch_{} dice=tensor(0.8931)\n",
            "val epoch_{} loss=0.1509854793548584\n",
            "val epoch_{} dice=tensor(0.2673)\n",
            "Epoch 127/199\n",
            "train epoch_{} loss=0.03394648879766464\n",
            "train epoch_{} dice=tensor(0.8928)\n",
            "val epoch_{} loss=0.15124189853668213\n",
            "val epoch_{} dice=tensor(0.2680)\n",
            "Epoch 128/199\n",
            "train epoch_{} loss=0.03240094222128391\n",
            "train epoch_{} dice=tensor(0.9036)\n",
            "val epoch_{} loss=0.15120837092399597\n",
            "val epoch_{} dice=tensor(0.2663)\n",
            "Epoch 129/199\n",
            "train epoch_{} loss=0.033029637113213536\n",
            "train epoch_{} dice=tensor(0.8947)\n",
            "val epoch_{} loss=0.15032809972763062\n",
            "val epoch_{} dice=tensor(0.2691)\n",
            "Epoch 130/199\n",
            "train epoch_{} loss=0.03467029109597206\n",
            "train epoch_{} dice=tensor(0.8785)\n",
            "val epoch_{} loss=0.14991647005081177\n",
            "val epoch_{} dice=tensor(0.2714)\n",
            "Epoch 131/199\n",
            "train epoch_{} loss=0.032535355910658835\n",
            "train epoch_{} dice=tensor(0.9020)\n",
            "val epoch_{} loss=0.15017473697662354\n",
            "val epoch_{} dice=tensor(0.2705)\n",
            "Epoch 132/199\n",
            "train epoch_{} loss=0.031736407056450845\n",
            "train epoch_{} dice=tensor(0.9046)\n",
            "val epoch_{} loss=0.1499861776828766\n",
            "val epoch_{} dice=tensor(0.2717)\n",
            "Epoch 133/199\n",
            "train epoch_{} loss=0.03398353569209576\n",
            "train epoch_{} dice=tensor(0.8916)\n",
            "val epoch_{} loss=0.15063324570655823\n",
            "val epoch_{} dice=tensor(0.2696)\n",
            "Epoch 134/199\n",
            "train epoch_{} loss=0.03253648839890957\n",
            "train epoch_{} dice=tensor(0.9027)\n",
            "val epoch_{} loss=0.15128649771213531\n",
            "val epoch_{} dice=tensor(0.2672)\n",
            "Epoch 135/199\n",
            "train epoch_{} loss=0.032361893355846404\n",
            "train epoch_{} dice=tensor(0.9028)\n",
            "val epoch_{} loss=0.1509137600660324\n",
            "val epoch_{} dice=tensor(0.2686)\n",
            "Epoch 136/199\n",
            "train epoch_{} loss=0.03324318639934063\n",
            "train epoch_{} dice=tensor(0.8913)\n",
            "val epoch_{} loss=0.15082988142967224\n",
            "val epoch_{} dice=tensor(0.2680)\n",
            "Epoch 137/199\n",
            "train epoch_{} loss=0.03268988989293575\n",
            "train epoch_{} dice=tensor(0.8955)\n",
            "val epoch_{} loss=0.15050677955150604\n",
            "val epoch_{} dice=tensor(0.2686)\n",
            "Epoch 138/199\n",
            "train epoch_{} loss=0.03178964741528034\n",
            "train epoch_{} dice=tensor(0.9054)\n",
            "val epoch_{} loss=0.15062163770198822\n",
            "val epoch_{} dice=tensor(0.2684)\n",
            "Epoch 139/199\n",
            "train epoch_{} loss=0.03261345624923706\n",
            "train epoch_{} dice=tensor(0.8966)\n",
            "val epoch_{} loss=0.1503206193447113\n",
            "val epoch_{} dice=tensor(0.2695)\n",
            "Epoch 140/199\n",
            "train epoch_{} loss=0.03222488686442375\n",
            "train epoch_{} dice=tensor(0.8996)\n",
            "val epoch_{} loss=0.15062224864959717\n",
            "val epoch_{} dice=tensor(0.2699)\n",
            "Epoch 141/199\n",
            "train epoch_{} loss=0.03203320875763893\n",
            "train epoch_{} dice=tensor(0.8980)\n",
            "val epoch_{} loss=0.15036413073539734\n",
            "val epoch_{} dice=tensor(0.2718)\n",
            "Epoch 142/199\n",
            "train epoch_{} loss=0.032657230645418166\n",
            "train epoch_{} dice=tensor(0.8991)\n",
            "val epoch_{} loss=0.15027648210525513\n",
            "val epoch_{} dice=tensor(0.2687)\n",
            "Epoch 143/199\n",
            "train epoch_{} loss=0.03346608951687813\n",
            "train epoch_{} dice=tensor(0.8949)\n",
            "val epoch_{} loss=0.15085682272911072\n",
            "val epoch_{} dice=tensor(0.2699)\n",
            "Epoch 144/199\n",
            "train epoch_{} loss=0.03332923576235771\n",
            "train epoch_{} dice=tensor(0.8912)\n",
            "val epoch_{} loss=0.1506906896829605\n",
            "val epoch_{} dice=tensor(0.2694)\n",
            "Epoch 145/199\n",
            "train epoch_{} loss=0.03128007873892784\n",
            "train epoch_{} dice=tensor(0.9015)\n",
            "val epoch_{} loss=0.14968442916870117\n",
            "val epoch_{} dice=tensor(0.2714)\n",
            "Epoch 146/199\n",
            "train epoch_{} loss=0.03366798423230648\n",
            "train epoch_{} dice=tensor(0.8845)\n",
            "val epoch_{} loss=0.14896196126937866\n",
            "val epoch_{} dice=tensor(0.2735)\n",
            "Epoch 147/199\n",
            "train epoch_{} loss=0.03239092156291008\n",
            "train epoch_{} dice=tensor(0.8963)\n",
            "val epoch_{} loss=0.1496066004037857\n",
            "val epoch_{} dice=tensor(0.2738)\n",
            "Epoch 148/199\n",
            "train epoch_{} loss=0.03365380391478538\n",
            "train epoch_{} dice=tensor(0.8886)\n",
            "val epoch_{} loss=0.14938928186893463\n",
            "val epoch_{} dice=tensor(0.2730)\n",
            "Epoch 149/199\n",
            "train epoch_{} loss=0.031481963396072385\n",
            "train epoch_{} dice=tensor(0.9077)\n",
            "val epoch_{} loss=0.15027184784412384\n",
            "val epoch_{} dice=tensor(0.2701)\n",
            "Epoch 150/199\n",
            "train epoch_{} loss=0.033157316595315935\n",
            "train epoch_{} dice=tensor(0.8963)\n",
            "val epoch_{} loss=0.15051734447479248\n",
            "val epoch_{} dice=tensor(0.2686)\n",
            "Epoch 151/199\n",
            "train epoch_{} loss=0.03214159607887268\n",
            "train epoch_{} dice=tensor(0.9014)\n",
            "val epoch_{} loss=0.1503203809261322\n",
            "val epoch_{} dice=tensor(0.2687)\n",
            "Epoch 152/199\n",
            "train epoch_{} loss=0.031801240518689156\n",
            "train epoch_{} dice=tensor(0.9033)\n",
            "val epoch_{} loss=0.1506001055240631\n",
            "val epoch_{} dice=tensor(0.2682)\n",
            "Epoch 153/199\n",
            "train epoch_{} loss=0.031839214637875556\n",
            "train epoch_{} dice=tensor(0.9048)\n",
            "val epoch_{} loss=0.15081971883773804\n",
            "val epoch_{} dice=tensor(0.2697)\n",
            "Epoch 154/199\n",
            "train epoch_{} loss=0.031789270043373105\n",
            "train epoch_{} dice=tensor(0.9063)\n",
            "val epoch_{} loss=0.15132373571395874\n",
            "val epoch_{} dice=tensor(0.2671)\n",
            "Epoch 155/199\n",
            "train epoch_{} loss=0.033173198252916335\n",
            "train epoch_{} dice=tensor(0.8959)\n",
            "val epoch_{} loss=0.151332288980484\n",
            "val epoch_{} dice=tensor(0.2675)\n",
            "Epoch 156/199\n",
            "train epoch_{} loss=0.03471923992037773\n",
            "train epoch_{} dice=tensor(0.8864)\n",
            "val epoch_{} loss=0.15032890439033508\n",
            "val epoch_{} dice=tensor(0.2729)\n",
            "Epoch 157/199\n",
            "train epoch_{} loss=0.032194311171770094\n",
            "train epoch_{} dice=tensor(0.9028)\n",
            "val epoch_{} loss=0.15088020265102386\n",
            "val epoch_{} dice=tensor(0.2705)\n",
            "Epoch 158/199\n",
            "train epoch_{} loss=0.03303238488733769\n",
            "train epoch_{} dice=tensor(0.8971)\n",
            "val epoch_{} loss=0.151227667927742\n",
            "val epoch_{} dice=tensor(0.2673)\n",
            "Epoch 159/199\n",
            "train epoch_{} loss=0.03346575722098351\n",
            "train epoch_{} dice=tensor(0.8902)\n",
            "val epoch_{} loss=0.15047897398471832\n",
            "val epoch_{} dice=tensor(0.2691)\n",
            "Epoch 160/199\n",
            "train epoch_{} loss=0.031522680819034574\n",
            "train epoch_{} dice=tensor(0.9058)\n",
            "val epoch_{} loss=0.15036600828170776\n",
            "val epoch_{} dice=tensor(0.2684)\n",
            "Epoch 161/199\n",
            "train epoch_{} loss=0.03241133689880371\n",
            "train epoch_{} dice=tensor(0.8979)\n",
            "val epoch_{} loss=0.15042132139205933\n",
            "val epoch_{} dice=tensor(0.2700)\n",
            "Epoch 162/199\n",
            "train epoch_{} loss=0.03296223133802414\n",
            "train epoch_{} dice=tensor(0.8966)\n",
            "val epoch_{} loss=0.15062588453292847\n",
            "val epoch_{} dice=tensor(0.2682)\n",
            "Epoch 163/199\n",
            "train epoch_{} loss=0.03241523839533329\n",
            "train epoch_{} dice=tensor(0.9006)\n",
            "val epoch_{} loss=0.15110856294631958\n",
            "val epoch_{} dice=tensor(0.2658)\n",
            "Epoch 164/199\n",
            "train epoch_{} loss=0.03505901396274567\n",
            "train epoch_{} dice=tensor(0.8771)\n",
            "val epoch_{} loss=0.15067294239997864\n",
            "val epoch_{} dice=tensor(0.2698)\n",
            "Epoch 165/199\n",
            "train epoch_{} loss=0.031924860924482344\n",
            "train epoch_{} dice=tensor(0.8996)\n",
            "val epoch_{} loss=0.15069685876369476\n",
            "val epoch_{} dice=tensor(0.2702)\n",
            "Epoch 166/199\n",
            "train epoch_{} loss=0.03223649188876152\n",
            "train epoch_{} dice=tensor(0.9012)\n",
            "val epoch_{} loss=0.1505829095840454\n",
            "val epoch_{} dice=tensor(0.2709)\n",
            "Epoch 167/199\n",
            "train epoch_{} loss=0.03164557851850987\n",
            "train epoch_{} dice=tensor(0.9021)\n",
            "val epoch_{} loss=0.15001240372657776\n",
            "val epoch_{} dice=tensor(0.2716)\n",
            "Epoch 168/199\n",
            "train epoch_{} loss=0.03326003700494766\n",
            "train epoch_{} dice=tensor(0.8929)\n",
            "val epoch_{} loss=0.15009179711341858\n",
            "val epoch_{} dice=tensor(0.2734)\n",
            "Epoch 169/199\n",
            "train epoch_{} loss=0.03180697523057461\n",
            "train epoch_{} dice=tensor(0.9049)\n",
            "val epoch_{} loss=0.15059533715248108\n",
            "val epoch_{} dice=tensor(0.2713)\n",
            "Epoch 170/199\n",
            "train epoch_{} loss=0.03254680521786213\n",
            "train epoch_{} dice=tensor(0.8994)\n",
            "val epoch_{} loss=0.15121109783649445\n",
            "val epoch_{} dice=tensor(0.2686)\n",
            "Epoch 171/199\n",
            "train epoch_{} loss=0.03175056166946888\n",
            "train epoch_{} dice=tensor(0.9035)\n",
            "val epoch_{} loss=0.15091776847839355\n",
            "val epoch_{} dice=tensor(0.2680)\n",
            "Epoch 172/199\n",
            "train epoch_{} loss=0.03188242726027966\n",
            "train epoch_{} dice=tensor(0.9028)\n",
            "val epoch_{} loss=0.15066491067409515\n",
            "val epoch_{} dice=tensor(0.2704)\n",
            "Epoch 173/199\n",
            "train epoch_{} loss=0.03255599476397038\n",
            "train epoch_{} dice=tensor(0.9020)\n",
            "val epoch_{} loss=0.15159545838832855\n",
            "val epoch_{} dice=tensor(0.2652)\n",
            "Epoch 174/199\n",
            "train epoch_{} loss=0.03397287391126156\n",
            "train epoch_{} dice=tensor(0.8898)\n",
            "val epoch_{} loss=0.15118566155433655\n",
            "val epoch_{} dice=tensor(0.2653)\n",
            "Epoch 175/199\n",
            "train epoch_{} loss=0.03214123174548149\n",
            "train epoch_{} dice=tensor(0.9034)\n",
            "val epoch_{} loss=0.15119653940200806\n",
            "val epoch_{} dice=tensor(0.2646)\n",
            "Epoch 176/199\n",
            "train epoch_{} loss=0.03189308010041714\n",
            "train epoch_{} dice=tensor(0.9034)\n",
            "val epoch_{} loss=0.15116575360298157\n",
            "val epoch_{} dice=tensor(0.2653)\n",
            "Epoch 177/199\n",
            "train epoch_{} loss=0.033781688660383224\n",
            "train epoch_{} dice=tensor(0.8917)\n",
            "val epoch_{} loss=0.15164805948734283\n",
            "val epoch_{} dice=tensor(0.2641)\n",
            "Epoch 178/199\n",
            "train epoch_{} loss=0.0328957237303257\n",
            "train epoch_{} dice=tensor(0.8946)\n",
            "val epoch_{} loss=0.15106144547462463\n",
            "val epoch_{} dice=tensor(0.2676)\n",
            "Epoch 179/199\n",
            "train epoch_{} loss=0.034272630140185355\n",
            "train epoch_{} dice=tensor(0.8863)\n",
            "val epoch_{} loss=0.15121522545814514\n",
            "val epoch_{} dice=tensor(0.2683)\n",
            "Epoch 180/199\n",
            "train epoch_{} loss=0.0313953097909689\n",
            "train epoch_{} dice=tensor(0.9028)\n",
            "val epoch_{} loss=0.15042495727539062\n",
            "val epoch_{} dice=tensor(0.2691)\n",
            "Epoch 181/199\n",
            "train epoch_{} loss=0.03325602523982525\n",
            "train epoch_{} dice=tensor(0.8974)\n",
            "val epoch_{} loss=0.15128040313720703\n",
            "val epoch_{} dice=tensor(0.2664)\n",
            "Epoch 182/199\n",
            "train epoch_{} loss=0.03152873814105987\n",
            "train epoch_{} dice=tensor(0.9027)\n",
            "val epoch_{} loss=0.15074853599071503\n",
            "val epoch_{} dice=tensor(0.2705)\n",
            "Epoch 183/199\n",
            "train epoch_{} loss=0.033187877759337424\n",
            "train epoch_{} dice=tensor(0.8996)\n",
            "val epoch_{} loss=0.15135332942008972\n",
            "val epoch_{} dice=tensor(0.2661)\n",
            "Epoch 184/199\n",
            "train epoch_{} loss=0.032980877161026004\n",
            "train epoch_{} dice=tensor(0.8945)\n",
            "val epoch_{} loss=0.15097466111183167\n",
            "val epoch_{} dice=tensor(0.2685)\n",
            "Epoch 185/199\n",
            "train epoch_{} loss=0.03229618892073631\n",
            "train epoch_{} dice=tensor(0.9042)\n",
            "val epoch_{} loss=0.1514897644519806\n",
            "val epoch_{} dice=tensor(0.2657)\n",
            "Epoch 186/199\n",
            "train epoch_{} loss=0.03312184289097786\n",
            "train epoch_{} dice=tensor(0.9006)\n",
            "val epoch_{} loss=0.15168818831443787\n",
            "val epoch_{} dice=tensor(0.2666)\n",
            "Epoch 187/199\n",
            "train epoch_{} loss=0.03169746361672878\n",
            "train epoch_{} dice=tensor(0.9043)\n",
            "val epoch_{} loss=0.1509416699409485\n",
            "val epoch_{} dice=tensor(0.2704)\n",
            "Epoch 188/199\n",
            "train epoch_{} loss=0.0319948960095644\n",
            "train epoch_{} dice=tensor(0.9040)\n",
            "val epoch_{} loss=0.15099895000457764\n",
            "val epoch_{} dice=tensor(0.2696)\n",
            "Epoch 189/199\n",
            "train epoch_{} loss=0.0330274187028408\n",
            "train epoch_{} dice=tensor(0.8943)\n",
            "val epoch_{} loss=0.15097087621688843\n",
            "val epoch_{} dice=tensor(0.2686)\n",
            "Epoch 190/199\n",
            "train epoch_{} loss=0.03251464813947678\n",
            "train epoch_{} dice=tensor(0.8999)\n",
            "val epoch_{} loss=0.15120035409927368\n",
            "val epoch_{} dice=tensor(0.2665)\n",
            "Epoch 191/199\n",
            "train epoch_{} loss=0.03195929266512394\n",
            "train epoch_{} dice=tensor(0.9030)\n",
            "val epoch_{} loss=0.15085726976394653\n",
            "val epoch_{} dice=tensor(0.2681)\n",
            "Epoch 192/199\n",
            "train epoch_{} loss=0.034143433719873426\n",
            "train epoch_{} dice=tensor(0.8863)\n",
            "val epoch_{} loss=0.15028513967990875\n",
            "val epoch_{} dice=tensor(0.2736)\n",
            "Epoch 193/199\n",
            "train epoch_{} loss=0.03250068835914135\n",
            "train epoch_{} dice=tensor(0.8980)\n",
            "val epoch_{} loss=0.15069043636322021\n",
            "val epoch_{} dice=tensor(0.2715)\n",
            "Epoch 194/199\n",
            "train epoch_{} loss=0.034414350241422656\n",
            "train epoch_{} dice=tensor(0.8845)\n",
            "val epoch_{} loss=0.1506042182445526\n",
            "val epoch_{} dice=tensor(0.2699)\n",
            "Epoch 195/199\n",
            "train epoch_{} loss=0.03256897702813148\n",
            "train epoch_{} dice=tensor(0.8950)\n",
            "val epoch_{} loss=0.14994069933891296\n",
            "val epoch_{} dice=tensor(0.2715)\n",
            "Epoch 196/199\n",
            "train epoch_{} loss=0.0324175912886858\n",
            "train epoch_{} dice=tensor(0.8980)\n",
            "val epoch_{} loss=0.14973050355911255\n",
            "val epoch_{} dice=tensor(0.2708)\n",
            "Epoch 197/199\n",
            "train epoch_{} loss=0.03114200159907341\n",
            "train epoch_{} dice=tensor(0.9087)\n",
            "val epoch_{} loss=0.15002082288265228\n",
            "val epoch_{} dice=tensor(0.2697)\n",
            "Epoch 198/199\n",
            "train epoch_{} loss=0.032152454182505605\n",
            "train epoch_{} dice=tensor(0.9039)\n",
            "val epoch_{} loss=0.15050354599952698\n",
            "val epoch_{} dice=tensor(0.2685)\n",
            "Epoch 199/199\n",
            "train epoch_{} loss=0.03395402953028679\n",
            "train epoch_{} dice=tensor(0.8746)\n",
            "val epoch_{} loss=0.14984187483787537\n",
            "val epoch_{} dice=tensor(0.2714)\n",
            "Epoch 200/199\n",
            "train epoch_{} loss=0.03174522519111633\n",
            "train epoch_{} dice=tensor(0.9041)\n",
            "val epoch_{} loss=0.15014150738716125\n",
            "val epoch_{} dice=tensor(0.2701)\n",
            "checkpoints/model_epoch_200.pth saved!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcBfzdm1Sx9Z"
      },
      "source": [
        "#Inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAwGU95AgmN-"
      },
      "source": [
        "import os\n",
        "src=\"/content/drive/MyDrive/Colab Notebooks/spermdata/spermdata/\"\n",
        "os.chdir(src)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkJrg3MVTT6R"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.autograd import Variable\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import os\n",
        "import cv2\n",
        "import sys\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "data_transforms =  transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize([0.5,0.5,0.5], [0.5,0.5,0.5])])\n",
        "\n",
        "modelpath =\"./checkpoints/model_epoch_200.pth\"\n",
        "net = torch.load(modelpath,map_location='cpu')\n",
        "net.eval()\n",
        "\n",
        "imagepaths = \"./test\"\n",
        "torch.no_grad()\n",
        "\n",
        "image = cv2.imread(\"./test/C2-Image2.tif\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4BRbNyNwZfg",
        "outputId": "02708994-179e-41d3-c60f-3c6a7d0a6ce4"
      },
      "source": [
        "imgblob = data_transforms(image).unsqueeze(0)\n",
        "predict = net(imgblob).cpu().data.numpy().copy()\n",
        "predict = predict>0.5\n",
        "result = np.squeeze(predict)\n",
        "\n",
        "print(np.max(result))\n",
        "result = (result*255).astype(np.uint8)\n",
        "print(result)\n",
        "resultimage = image.copy()\n",
        "cv2.imwrite(\"./test/C2-Image2_result.tif\",result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYm5iyMi4xAi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}